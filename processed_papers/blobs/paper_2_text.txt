Here is all the text content extracted from the provided OCR output, page by page:

==Start of OCR for page 1==
arXiv:2504.21659v1 [cs.AI] 30 Apr 2025
AdaR1: From Long-CoT to Hybrid-CoT via
Bi-Level Adaptive Reasoning Optimization
Haotian Luo¹, Haiying He2* Yibo Wang³, Jinluan Yang¹, Rui Liu
Naiqiang Tan, Xiaochun Cao¹, Dacheng Tao, Li Shen¹†
1 Sun Yat-sen University; 2 China Agricultural University; 3 Tsinghua University;
4 Zhejiang University; 5 Didichuxing Co. Ltd; 6 Nanyang Technological University
Abstract
Recently, long-thought reasoning models achieve strong performance on complex
reasoning tasks, but often incur substantial inference overhead, making efficiency a
critical concern. Our empirical analysis reveals that the benefit of using Long-CoT
varies across problems: while some problems require elaborate reasoning, others
show no improvement or even degraded accuracy. This motivates adaptive reason-
ing strategies that tailor reasoning depth to the input. However, prior work primarily
reduces redundancy within long reasoning paths, limiting exploration of more ef-
ficient strategies beyond the Long-CoT paradigm. To address this, we propose a
novel two-stage framework for adaptive and efficient reasoning. First, we construct
a hybrid reasoning model by merging long and short CoT models to enable diverse
reasoning styles. Second, we apply bi-level preference training to guide the model
to select suitable reasoning styles (group-level), and prefer concise and correct
reasoning within each style group (instance-level). Experiments demonstrate that
our method significantly reduces inference costs compared to other baseline ap-
proaches, while maintaining performance. Notably, on five mathematical datasets,
the average length of reasoning is reduced by more than 50%, highlighting the
potential of adaptive strategies to optimize reasoning efficiency in large language
models. Our code is coming soon at https://github.com/StarDewXXX/AdaR1
1 Introduction
Recent advancements in large language models (LLMs), such as OpenAI's O1[1] and Deepseek's
R1[2], feature the adoption of detailed and complex reasoning processes (Long-CoT) analogous
to human deliberation on complex problems. This reasoning paradigm demonstrably enhances the
models' problem-solving capabilities and yields promising results. However, the enhanced reasoning
capability derived from Long-CoT often comes at substantial inference overhead. Generating lengthy
sequences of intermediate reasoning steps incurs significant computational costs, increases latency,
and consumes considerable resources[3-5]. This efficiency bottleneck poses a critical challenge
for deploying these powerful reasoning models in real-world, resource-constrained applications or
interactive scenarios where responsiveness is paramount.
Existing approaches to improving reasoning efficiency often focus on optimizing within the Long-
CoT distribution. Techniques such as O1-Pruner [6], Training and Overthinking compress reasoning
paths with various methods but they are inherently limited because they still choose long-thought
distribution as the starting point. They primarily address redundancy within a potentially unnecessarily
long path, rather than focusing whether such a long path was needed in the first place. This leaves
*Equal contribution
Corresponding Author: Li Shen (shenli6@mail.sysu.edu.cn)
Preprint. Under review.
==End of OCR for page 1==

==Start of OCR for page 2==
unexplored the potential efficiency gains from fundamentally choosing a different, more concise
reasoning strategy altogether when appropriate. CoT-Valve[7] allows models to produce both extended
and concise responses simultaneously; however, it lacks mechanisms for adaptively selecting the
optimal reasoning path contingent on the characteristics of the input problem, which brings degraded
performance.
Our investigation (presented in Section 3) about the benefit of Long-CoT reasoning reveals a crucial
insight: the utility of long, elaborate reasoning chains is highly problem-dependent. While complex
problems genuinely benefit from detailed, step-by-step derivations, many other problems can be
solved accurately and more efficiently with shorter, more direct reasoning paths. In fact, for simpler
problems, forcing a Long-CoT process might not only be wasteful but can sometimes even introduce
errors or degrade performance. This observation strongly motivates the need for adaptive reasoning
strategies – systems that can tailor the depth and style of their reasoning process to the specific
demands of the input problem.
Inspired by this, we propose a novel two-stage framework designed to enable efficient and adaptive
reasoning. Our core idea is to move beyond optimizing a single reasoning style and instead equip the
model with the flexibility to choose between different reasoning approaches. First, we construct a
hybrid reasoning model by merging long and short CoT models to enable diverse reasoning styles.
Second, we introduce a specialized training methodology called Bi-Level Adaptive Reasoning Optimization. This
training process operates on two levels: (i) Group-Level Preference: It guides the hybrid model to
implicitly discern the overall complexity of a given problem and select the most appropriate reasoning
style (e.g., Long-CoT for complex tasks, Short-CoT for simpler ones). (ii) Instance-Level Preference:
Within the chosen style group (long or short), it further refines the model's output by encouraging it
to prefer reasoning paths that are not only correct but also concise.
By training the model to make intelligent choices at both the style-selection level and the instance-
generation level, our framework aims to dynamically allocate computational resources where they are
most needed. As demonstrated through our experiments, this approach leads to significant reductions
in inference costs compared to standard Long-CoT models and other efficiency-focused baselines,
while maintaining strong reasoning performance. Specifically, in MATH[8], the reasoning length
decreases by 58% while maintaining accuracy, while in GSM8K[9], the length decreases by 74%
alongside an improvement in accuracy. This work underscores the promise of adaptive strategies as a
key direction for optimizing the trade-off between reasoning quality and computational efficiency of
large reasoning models.
Our contributions can be summarized as follows:
• We conduct an empirical analysis investigating the benefits of long Chain-of-Thought
(CoT) reasoning relative to shorter CoT approaches, identifying the conditions under which
extended reasoning paths offer tangible advantages.
• We propose using Adaptive Hybrid Reasoning Model to enhance inference efficiency,
accompanied by a novel training pipeline (AdaR1). Comprehensive experiments demonstrate
that our proposed method achieves excellent performance, significantly improving efficiency
while maintaining high accuracy.
• We perform further analyses on the resulting Adaptive Hybrid Reasoning Model to gain
deeper insights into its characteristics and operational behavior. And we will release the
model weights of the Adaptive Hybrid Reasoning Model to the public to encourage further
research and application by the community.
2 Related Work
2.1 Model Merging
Model merging[10] is an emerging technique that combines the parameters of multiple trained
models into a single one without requiring access to the original training data. Unlike ensemble
learning[11], which aggregates outputs at inference time, model merging operates at the parameter
level, resulting in a more efficient and compact model. Recent researchers have proposed various
merging methods, including parameter interpolation[12] and alignment-based strategies[13], and
2
==End of OCR for page 2==

==Start of OCR for page 3==
explored their applications in large language models, multimodal language models and many subfields
in machine learning. Beyond simple linear merging, which averages the model parameters, researchers
have proposed more advanced methods such as DARE[14], TIES-Merging[15], and AdaMerging[16].
DARE reduces delta parameter redundancy by dropping and rescaling parameters before merging.
TIES-Merging focus on reducing interference from redundant parameters and sign disagreements by
trimming, electing parameter signs, and performing disjoint merges. AdaMerging adaptively adjusts
task or layer weights via entropy minimization on unlabeled test samples to enhance merged model
performance without original training data.
Different from traditional model merging, which integrates parameters from multiple source models
to consolidate learned capabilities, our work try to empower a single model to adaptively select an
appropriate reasoning mode (Long-CoT or Short-CoT) for each problem instance, primarily targeting
the optimization of computational efficiency in reasoning rather than multi-task performance.
2.2 Alignment
LLM alignment[17] research aims to ensure that large language models generate responses that
meet human expectations, enabling us to utilize these powerful language models in a reliable
and responsible way. Reinforcement Learning from Human Feedback (RLHF)[18] is one of the
mainstream methods, which optimizes the model based on human feedback, but its high computational
cost has led researchers to explore more efficient alignment techniques, such as Direct Preference
Optimization (DPO)[19], which simplify the alignment process by directly optimizing the LLM
policy based on preference data. other approaches such as Reinforcement Learning from AI Feedback
(RLAIF)[20] utilize AI-generated feedback to reduce costs and enhance alignment efficiency. These
diverse techniques reflect the dynamic evolution of the field, with the aim of improving both the
effectiveness and efficiency of LLM alignment.
Although we use preference learning, our method diverges from the objectives of LLM alignment.
While alignment techniques focus on shaping model outputs and behavior to conform with human
preferences, values, and safety criteria, our research optimizes the internal reasoning generation
process-specifically, the Chain-of-Thought. The aim is to enhance problem-solving efficiency and
accuracy rather than aligning model behavior with human judgment.
3 Motivation
3.1 Problem Setup
Chain-of-Thought (CoT) prompting has emerged as a powerful technique for enhancing the reasoning
capabilities of large language models. Within the CoT paradigm, a distinction can be made between
Long-CoT, which involves generating detailed and extensive thinking steps, and Short-CoT, which
directly generate solving steps.
3.2 When Do We Need Long-CoT?
Simply applying Long-CoT uniformly across all problems leads to unnecessary overhead on instances
where its detailed steps are not required or do not yield accuracy gains. Therefore, we empirically
investigate the necessity of Long-CoT and analyze the relationship between problem characteristics
and Long-CoT efficacy in this subsection. We design an experiment to analyze the accuracy gains of
Long-CoT over Short-CoT across different types of problems. Specifically, we use DeepSeek-R1-
Distill-Qwen-7B as the Long-CoT model. For the Short-CoT model, we fine-tune the DeepSeek-R1-
Distill-Qwen-7B using 2,000 Short-CoT samples generated by Qwen2.5-Math-7B-Instruct[21]. We
do not directly use Qwen2.5-Math-7B-Instruct as the Short-CoT model, since its training template
differs substantially from that of the DeepSeek-Distill series, which could introduce unpredictable
effect in the subsequent model merging and sampling process. We construct a mixed dataset (called
MixMathematics) which comprises training examples from AIME[22], MATH, and GSM8K. Further
datasets details can be found in the Section 5.1.
We selected 2,500 samples and generated 12 responses for each question. To refine our analysis
and focus on solvable instances, we first removed samples where both the long and short CoT
accuracies were zero, as such cases typically indicate problems exceeding the model's current
3
==End of OCR for page 3==

==Start of OCR for page 4==
1
Gain
Ratio
0.8
0.6
0.4
>0
48%
0.2
0
-0.2
=0
45%
-0.4
-0.6
-0.8
<0
7%
-1
Ok-1k
1k-2k
2k-4k
4k-6k
gain>0 gain≤0
6k-8k
8k-16k
Proportion of Accuracy Diff by Length Bins
Figure 1: The proportion of gain in the data (left) and the relationship between CoT length and
accuracy improvement (right), Long-CoT reasoning improves accuracy on difficult problems but has
little effect or harms performance on easy ones.
Method
Performance (Accuracy)
Optimization Scope
Overthinking[4]
Limited x
Slightly Dropped ✓
kimi-1.5[23]
Limited x
01-Pruner
Limited x
Slightly Dropped ✓
Not Dropped ✓
Naive Merge
Broad✓
(mostly) Dropped ×
CoT-Valve
Broad✓
Dropped ×
AdaR1(Ours)
Broad✓
Slightly Dropped ✓
Table 1: Comparison of Different Methods. The term "limited" refers to an optimization scope
where the model operates within the distribution of long-CoT, inherently limiting its capacity for
efficiency. In contrast, "broader" denotes an optimization approach that encompasses both long-CoT
and short-CoT, allowing the model to achieve efficiency by generating shorter responses without
thinking. "slightly dropped" indicates a reduction in accuracy of less than 3%. Conversely, the
designation "dropped" signifies a more substantial performance degradation, corresponding to a
decrease in accuracy exceeding 3%.
capabilities. Subsequently, we calculated the accuracy for both the long and short CoT samples and
then determined the accuracy gain (i.e., the accuracy of the Long-CoT minus the accuracy of the
Short-CoT). Our analysis proceeded in two phases. First, as depicted in the left portion of Figure 1,
we quantified the proportion of samples falling into three categories based on the accuracy gain: gain
> 0, gain = 0, and gain < 0. Notably, we observed that for nearly half of the samples, employing a
Long-CoT did not yield any improvement, and in a small fraction of cases, it even led to a decrease
in performance. To further investigate this phenomenon, we categorized the samples based on the
average length of their Long-CoT reasoning, as length is often correlated with problem difficulty. The
right portion of Figure 1 reveals that the benefit derived from Long-CoT becomes more pronounced
for problems with greater length (and thus, presumably higher difficulty). In the shortest length
interval, Long-CoT offered negligible gains, whereas in the longest length interval, it resulted in a
substantial improvement in accuracy.
Prior methods (Table 1), such as Overthinking [4], kimi-1.5 [23], and O1-Pruner, typically operate
within a limited optimization scope but generally maintain performance stability or incur only a slight
drop, with O1-Pruner notably achieving no performance decrease. In contrast, methods designed
for a broad optimization scope, including Model Merge and CoT-Valve, did not consider how to
tackle easy and different problems, rendering the model incapable of determining its reasoning depth
according to the inherent difficulty of the task. Thus they frequently result in significant performance
degradation. In a nutshell, methods with a restricted optimization can generally preserve performance
but lose the chance to utilize shorter CoT. However, approaches capable of utilize broader CoT
distribution have struggled to maintain accuracy due to their inability to adapt adequate reasoning
depth to problem complexity.
The finding mentioned in last section motivates us to address the efficiency challenge of Long-CoT
models from a novel perspective: enabling the reasoning model to adaptively select an appropriate
4
==End of OCR for page 4==

==Start of OCR for page 5==
reasoning mode (long or short CoT) for different problems, and then generate a correct and concise
CoT in the determined mode. Our proposed method (AdaR1) differentiates itself by successfully
achieving a broad optimization scope while incurring only a marginal performance decrement. This
demonstrates a more favorable trade-off between efficiency and accuracy compared to existing
broad-scope optimization techniques.
4 Bi-Level Adaptive Reasoning Optimization
4.1 Problem Setup
We consider a LLM parameterized by @ and denoted as πθ. In the context of math problem solving,
the LLM accepts a sequence x = [x¹, . . ., xn], commonly termed as the problem, and then generate
a corresponding solution y = [y¹, . . ., ym]. Hence, the solution y is construed as a sample drawn
from the conditional probability distribution πο(·|x). The conditional probability distribution πο(y|x)
can be decomposed as follows:
m
πο(y|x) = Ππο(y|x, y<i).
j=1
(1)
We consider two LLMs: one trained to generate long, reflective Chain-of-Thought (CoT) reasoning
(Long-CoT model, denoted as θ₁) and the other trained for short and concise reasoning paths (Short-
CoT model, denoted as 0s). These two models are typically fine-tuned with different CoT and
demonstrate distinct reasoning patterns.
4.2 Method Overview
Our method consists of two stages, shown in Figure 2. First, we merge a Long-CoT model and a
Short-CoT model to obtain a unified reasoning model capable of generating both types of reasoning
paths. This allows exploration over a broader CoT distribution. In the second stage, we apply
Bi-Level Preference Training: for group-level preference, the model learns to choose between long
and short reasoning group based on the input; for instance-level preference, it learns to compress the
reasoning path to improve efficiency within the chosen group determined by group-level preference.
πθι
Merge, M
πθς
Stage I
πθΗ
πθι
πθς
Sample
Sample
Pgroup
Bi-Level
Optimization
Pinstance
Stage II
+ M Train, M
M=M
πθη
πθΗ
Figure 2: Pipeline of AdaR1. At Stage I, we fused the models to obtain πөн. In Stage II, we sample
from both long and short models and then elicit the group-level and instance-level preference. After
this, we optimize πөн at both group and instance level to obtain a hybrid adaptive reasoning model.
4.3 Stage I: Long-and-Short Reasoning Merge
To enable flexible reasoning behaviors within a single model, we first perform model merging with
long and short models. We adopt a simple yet effective strategy of linearly merging their parameters.
Given two models with parameters θ₁ and 0s, we compute the merged model as:
θη = αθι + (1 − α)θς,
(2)
5
==End of OCR for page 5==

==Start of OCR for page 6==
where a ∈ [0,1] is a merging coefficient that balances the contribution from each model. The
resulting hybrid reasoning model, πөн, inherits the capacity to generate both long and short CoT
depending on the input.
This merged model expands the diversity of the CoT distribution it can produce, laying the foundation
for adaptive reasoning. By combining the strengths of both reasoning styles, it enables the model
to potentially match different problem types with suitable reasoning strategies, which is key to
improving efficiency in the next stage.
4.4 Stage II: Bi-Level Preference Training
In this stage, we introduce a Bi-Level Preference Training strategy to fine-tune the model toward
efficient reasoning. The core idea is to train the model to: (1) select the appropriate reasoning style
(long or short) for each problem (group-level preference) and (2) further compress the reasoning
within the determined chosen group (instance-level preference).
Group Labels. We define a group label g to denote the reasoning style of a response group. Let
GL denote the long reasoning group and gs denote the short reasoning group. For a given input
problem x, a generated resposne (solution) y belongs to one of the two groups. We use {Yi}g=gL to
denote the set of K Long-CoT responses generated by the Long-CoT model 01, and {yj}g=gs for
the corresponding short responses from the Short-CoT model 0s.
K
SK
Group-Level Preference. For each math problem x in the dataset D, we sample K solutions from
both the long and short reasoning models. Let {y{}, and {y}-1 be the respective sample sets.
We define the approximated accuracy expectation for each group as:
[CL(x)] =
1
K
1
K
Σ 1[Correct(y)], [CS(x)] = = 1 [Correct(y)],
i=1
j=1
(3)
where 1[.] is the indicator function. Then we introduce a preference margin threshold € > 0. The
group-level preference for x is then determined as:
9L > gsx if Ê[CL(x)] – Ê[CS(x)] > €,
(gs>9Lx if Ê[CL(x)] – Ê[CS(x)] ≤ €.
Given the group-level preference for an input x, we form training pairs from the Cartesian product of
the two groups. For example, if g₁ > gs x, we construct the preference pairs as:
Pgroup(x) = {(y, y;) | i ∈ [1, K], j ∈ [1, K]} .
(4)
From this set of pairs, we randomly sample a subset contain M₁ pairs to construct DPO training
tuples (x, yw, Yı), where yw is the preferred (chosen) response and yī is the less preferred (rejected).
For all x ∈ D, we perform group-level preference assignment by comparing the sampled long and
short responses as described above. These tuples are then aggregated into a new dataset Dgroup
{(x, yw, Yı)}, which serves as supervision for optimizing the DPO objective at the group level.
=
Instance-Level Preference. Once the preferred group g* ∈ {gL, 9S} is determined for a given x,
we further construct instance-level preferences within that group to encourage more concise reasoning.
We compare response pairs (Ya, Y♭) such that both belong to the same group (e.g., Ya, Yb ∈ {y}),
and prefer the shortest correct response. For dispreferred samples, we select M2 longest responses.
Formally, for each x ∈ D with preferred group g*, we first identify the subset of correct responses
{Yi}correct
{Yi}g=g*. Among these, we select the shortest correct response as the preferred instance:
Yw = arg min |y|.
YE{Y} correct
To construct instance-level preference pairs, we then select the M2 longest responses from the entire
group {yi}g=g*. Denote these as {Y1;} }21. This yields a dataset of instance-level training tuples:
Dinstance = {(X, Yw, Yl) | Yw
(M2)
arg miny∈ {y} correct |y|, y ∈ arg max yeyi}g=g*
Y}
6
==End of OCR for page 6==

==Start of OCR for page 7==
Bench
Model
7B Models
AIME25 MATH500 GSM8K Olympiad Minerva Avg.(%)
38.3
90.2
88.9
54.4
Long(R1-distill)
(11005)
(3534) (1014)
35.7
(7492) (4533)
10.0
78.6
89.5
39.4
28.6 -19.97%
Short
(957)
(591)
(272)
(910)
(579)
(-84.57%)
21.7
79.4
88.4
41.2
25.7
-18.63%
Merge
(9079)
(916)
(236)
(3743)
(1734)
(-56.02%)
35.8
89.4
86.0
55.2
35.6
-3.56%
DPO
(9976) (2334)
(360)
(5309)
(3281) (-33.26%)
40.0
92.4
89.4
55.3
35.3
+2.48%
O1-Pruner
(9353) (2212)
(377)
(5295)
(3259)
(-34.53%)
22.5
78.6
87.9
39.6
29.4
-18.41%
CoT-Valve
(5024)
(747)
(235)
(2313)
(629)
(-73.06%)
35.8
90.2
90.3
52.4
34.1
-1.65%
AdaR1(Ours)
(8426)
(1468)
(260)
(4889)
(1647)
(-50.93%)
1.5B Models
23.3
Long(R1-distill)
81.0
(12307) (4416)
80.9
41.6
26.1
(1481)
(7687)
(5789)
9.0
69.4
78.2
30.7
22.4
-26.34%
Short
(1098)
(740)
(269)
(1373)
(725)
(-85.15%)
20.8
71.8
74.2
28.6
20.0
-10.12%
Merge
(9226)
(1740)
(251)
(3767)
(1399)
(-59.10%)
20.8
81.4
74.8
42.8
24.3
-5.93%
DPO
(10224)
(3055)
(374)
(6319)
(3905)
(-34.57%)
23.3
82.6
84.6
44.7
28.3
+2.18%
O1-Pruner
(9496) (2782)
(726)
(5658)
(3964)
(-33.75%)
14.2
69.6
76.3
28.7
19.5
-19.61%
CoT-Valve
(7744)
(1299)
(205)
(3169)
(867)
(-67.52%)
AdaR1(Ours)
23.0
(9516)
80.8
79.2
42.1
23.5
-1.21%
(2455)
(341) (5802) (3021) (-43.28%)
Table 2: Accuracy (shown above) and length (shown below) of models and methods on different
benchmarks. Avg represents the change in length and accuracy compared to the Long model (+ for
increase, - for decrease).
These instance-level preferences encourage the model not only to reason correctly, but also to do so
concisely within the preferred reasoning style.
We sample such intra-group pairs and use them as additional training data for DPO to encourage the
model to favor more concise reasoning within each group.
Objective. Given collelcted preference datasets Dgroup and Dinstance sampled from p* which contains
N preference pairs (x, yw, Yı). With a parameter ẞ controlling the deviation from the reference model
Pref, DPO optimize the model by:
max E(x,yw,y1)~DgroupŪDinstance
πθΗ
[logo (Blog πθΗ (Yω |
σ
|x)
Blog
Πoref (Yw | X)
πθη (ψι|x)
Πoref (Yl | X)
7
==End of OCR for page 7==

==Start of OCR for page 8==
5 Experiments
5.1 Setup
Long-CoT Models. The long thought models we chosen for our experiment are DeepSeek-R1-Distill-
Qwen-7B and DeepSeek-R1-Distill-Qwen-1.5B, which have demonstrated excellent performance on
most math problem-solving tasks. For both models, we utilize full-parameter fine-tuning.
Short CoT Models. Since model merging requires Shot-CoT models, we face two issues with
existing Shot-CoT models: (1) they often employ templates that differ from those used in Long-
CoT models; (2) they tend to exhibit substantial parameter deviations from the base model, which
introduces instability during the merging process [16, 24]. To address these challenges, we fine-tune
the Long-CoT models using a small number of short CoT examples to obtain the corresponding
Shot-CoT models. This approach ensures consistency in template usage and maintains a closer
parameter proximity between the two models.
Dataset. Following s1[25] and Light-R1[26], we construct a mixed training dataset to ensure coverage
across mathematical problems of varying difficulty levels. Specifically, we combine GSM8K, MATH,
and AIME datasets in a ratio of 1:3:1, resulting in a total of 2,500 diverse math problems.
Evaluation. We use the GSM8K test set, the MATH test set, and AIME25 as in-distribution
evaluation data, while Olympiad[27] and Minerva[28] are employed as out-of-distribution test sets.
For evaluation metrics, we consider both accuracy and sequence length. Additionally, we report the
average accuracy degrade rate and the average length reduction rate across all test sets.
5.2 Competitive Methods
DPO. DPO are widely used baselines in reasoning optimization area. Follwoing the setting of [23, 4],
we choose shortest sample as chosen samples and longest sample as rejected sample.
CoT-Valve. CoT-Valve enables dynamic control of Chain-of-Thought length using a single model by
identifying and leveraging a controllable direction in the model's parameter space, reducing inference
cost by compressing reasoning paths based on task difficulty.
01-Pruner. 01-Pruner is a method designed to reduce reasoning overhead while maintaining model
accuracy. It begins by establishing a baseline through pre-sampling, and then applies reinforcement
learning-based finetuning to guide the model toward generating more concise reasoning under
accuracy-preserving constraints.
5.3 Main Results
We can be seen from the Table 2 that: the Short and Merge models achieve the most significant
length reduction compared to the Long Model. However, this efficiency gain is accompanied by a
notable degradation in accuracy, exceeding 10 percentage points. Among the models that do not
suffer significant accuracy degradation, our method achieves the best length reduction performance,
reaching 50.93% for the 7B model and 43.28% for the 1.5B model. Compared to DPO, our approach
demonstrates both more substantial length reduction and significantly less accuracy degradation.
While O1-Pruner maintains high accuracy, its length reduction effect is considerably weaker than that
of our method.
5.4 Ablation Study
To understand the contribution of each component of our proposed framework, we conduct an
ablation study. We evaluate several variants of our model on a subset of the benchmarks (AIME25,
MATH500, GSM8K). Table 3 presents the results. As shown, the Merge model achieves a significant
average length reduction of 56.10% compared to the baseline, but this comes at a substantial accuracy
degradation of 12.83%.
Next, we investigate the effect of Supervised Fine-Tuning (using the chosen sample in our group
level preference dataset) on the merged model. SFT helps recover a significant portion of the lost
accuracy, bringing the average degradation down to 3.82%. However, its average length reduction is
less pronounced (31.86%) compared to the Merge model without further training.
8
==End of OCR for page 8==

==Start of OCR for page 9==
Bench
AIME25 MATH500 GSM8K Avg.(%)
Model
38.3
90.2
88.9
Long(R1-distill)
(11005)
(3534) (1014)
21.7
79.4
88.4 -12.83%
Merge
(9079)
(916)
(236) (-56.10%)
35.8
84.6
88.7
-3.82%
Merge + SFT
(11222)
(2314)
(375) (-31.86%)
30.8
87.8
91.6
-3.31%
Merge + group level
(9049)
35.8
(1565)
90.2
(359)
90.3
(-46.03%)
-0.51%
Merge + bi level
(8426) (1468) (260) (-52.08%)
Table 3: Ablation study of each component on several benchmarks, showing that the Merge + bi-level
achieves the best trade-off, with a 52.08% average length reduction and a minimal 0.51% accuracy
degradation compared to others.
Introducing the group-level preference training after merging (Merge + group level) yields better
results than SFT. It achieves a higher average length reduction (46.03%) and a slightly better accuracy
recovery, with only a 3.31% average degradation relative to the baseline. This indicates that training
the model to select the appropriate reasoning style is effective in balancing efficiency and accuracy.
Finally, the full proposed method, incorporating both group and instance-level preference training
(Merge + bi level), demonstrates the most favorable trade-off. It achieves an impressive average
length reduction of 52.08% while recovering accuracy to within 0.51% of the baseline, significantly
outperforming all other ablated variants in terms of accuracy retention while maintaining substantial
efficiency gains. This result highlights the complementary benefits of the bi-level training approach:
the group level guides the model towards suitable reasoning styles, and the instance level further
refines the chosen style by favoring concise and correct responses, leading to a highly efficient and
accurate hybrid reasoning model.
6 Evaluation
In this section, we analyze the reasoning behavior of the obtained Hybrid Adaptive Reasoning Model
(AdaR1) to gain deeper insights into its characteristics.
6.1 Thinking Ratio Study
To investigate the thinking characteristics of different models, we propose the "Thinking Ratio"
metric. This metric is designed to detect whether a response constitutes a deep thinking (Long-CoT)
sample. Long-CoT responses typically include unique keywords (e.g., 'wait', 'recheck', 'hold on').
By detecting the presence of these keywords in a response, we can effectively determine if it is a deep
thinking sample. This detection method is more generalizable than relying solely on response length.
We considered a subset of the models from the main experiments and collected their responses on
the Math Testset. Using the method described above, we analyzed the proportion of deep thinking
samples for each model. Furthermore, for each category (thinking/non-thinking samples), we also
calculated their accuracy.
The results are shown in Figure 3. The baseline Long-CoT model predominantly employs deep
thinking (0.98), yielding high accuracy. In contrast, the Naive Merge model drastically shifts towards
non-thinking responses (0.94) but suffers significant accuracy degradation on both thinking (0.68)
and non-thinking (0.81) paths. DPO shows a moderate shift to non-thinking (0.34) while preserving
accuracy. Our AdaR1 model achieves a more significant shift towards non-thinking (0.72) than
DPO, yet crucially maintains high accuracy for these dominant non-thinking responses (0.96), unlike
the Naive Merge. This demonstrates AdaR1's effective adaptation, utilizing efficient shorter paths
without substantial accuracy loss.
9
==End of OCR for page 9==

==Start of OCR for page 10==
0.8
0.6
0.4
0.2
1
0
AdaR1
R1-7B
7B-DPO
Non-thinking COT
Non-thinking COT ACC
■Thinking COT
Thinking COT ACC
Merging
Figure 3: The proportion and accuracy of thinking and non-thinking in different methods, AdaR1 can
achieve a good balance and accuracy between thinking and non-thinking.
6.2 Adaptive Reasoning Study
This section evaluates the adaptive reasoning capability of AdaR1(7B) models on mathematical tasks
using the MATH dataset. The problems within the MATH dataset are categorized into five distinct
difficulty levels (Level 1 to Level 5), making it suitable for our study. We analyze the AdaR1 model's
thinking ratio (mentioned in last subsection) and average correctness across these five difficulty levels.
As illustrated in the left part of Figure 4, the AdaR1 model demonstrates a clear trend in the utilization
of thinking (Long-CoT) based on problem difficulty. A clear trend is observed where the proportion
of responses exhibiting long-CoT significantly increases with task difficulty. Specifically, Level
1 tasks show a very low thinking ratio, which progressively increases through intermediate levels,
reaching its highest ratio for Level 5 tasks. This suggests that the AdaR1 model adaptively engages
in thinking more frequently when confronted with more complex problems.
Regarding average correctness, as depicted in the right panel of Figure 4, the AdaR1 model generally
performs well across most difficulty levels. Its accuracy is comparable to that expected from a 'pure'
Long-CoT model (Deepseek-R1-Qwen-7B-Distill) and significantly surpasses that of Short-CoT
model, particularly across levels 3 to 5. This outcome aligns with our expectation presented in Section
3, suggesting that the model has learned to adaptively employ Long-CoT when beneficial, thereby
achieving a favorable trade-off between computational efficiency and performance.
Proportions
0.8
0.6
0.4
0.2
■ Thinking COT
Non-Thinking COT
1
Accuracy
0.75
1
AdaR1 --- Long
Short
0
0.5
Level 1
Level 2
Level 3
Level 4
Level 5
Level 1
Level 2
Level 3
Level 4
Level 5
Different Difficulty Levels
Different Difficulty Levels
Figure 4: The ratio of thinking and non-thinking CoTs of AdaR1-7B on different MATH levels (left)
and the accuracy on different MATH levels of different models (right). As the difficulty increases,
AdaR1 is able to think more on harder problems and maintain higher accuracy.
10
==End of OCR for page 10==

==Start of OCR for page 11==
7 Conclusion
In this paper, we demonstrate through empirical analysis that the benefits of Long-CoT reasoning
vary significantly depending on the problem. Motivated by this, we propose a novel two-stage
training framework for adaptive reasoning. Experiments show that model trained with our method can
reason adaptively to different problems. And our method significantly reduces inference costs while
preserving performance, highlighting the promise of adaptive strategies for optimizing reasoning
efficiency in large language models.
References
[1] OpenAI.
Learning to reason with Ilms.
https://openai.com/index/
learning-to-reason-with-llms/, 2024. [Accessed 19-09-2024].
[2] DeepSeek-AI team. Deepseek-r1: Incentivizing reasoning capability in Ilms via reinforcement
learning, 2025.
[3] Jeffrey Cheng and Benjamin Van Durme. Compressed chain of thought: Efficient reasoning
through dense representations, 2024.
[4] Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi
Liu, Mengfei Zhou, Zhuosheng Zhang, Rui Wang, Zhaopeng Tu, Haitao Mi, and Dong Yu. Do
not think that much for 2+3=? on the overthinking of o1-like llms, 2025.
[5] Yang Sui, Yu-Neng Chuang, Guanchu Wang, Jiamu Zhang, Tianyi Zhang, Jiayi Yuan, Hongyi
Liu, Andrew Wen, Shaochen Zhong, Hanjie Chen, and Xia Hu. Stop overthinking: A survey on
efficient reasoning for large language models, 2025.
[6] Haotian Luo, Li Shen, Haiying He, Yibo Wang, Shiwei Liu, Wei Li, Naiqiang Tan, Xiaochun
Cao, and Dacheng Tao. Ol-pruner: Length-harmonizing fine-tuning for o1-like reasoning
pruning, 2025.
[7] Xinyin Ma, Guangnian Wan, Runpeng Yu, Gongfan Fang, and Xinchao Wang. Cot-valve:
Length-compressible chain-of-thought tuning, 2025.
[8] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn
Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset.
NeurIPS, 2021.
[9] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,
Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John
Schulman. Training verifiers to solve math word problems, 2021.
[10] Enneng Yang, Li Shen, Guibing Guo, Xingwei Wang, Xiaochun Cao, Jie Zhang, and Dacheng
Tao. Model merging in llms, mllms, and beyond: Methods, theories, applications and opportu-
nities, 2024.
[11] Yichong Huang, Xiaocheng Feng, Baohang Li, Yang Xiang, Hui Wang, Bing Qin, and Ting Liu.
Ensemble learning for heterogeneous large language models with deep parallel collaboration,
2024.
[12] Yue Zhou, Yi Chang, and Yuan Wu. Mixup model merge: Enhancing model merging perfor-
mance through randomized linear interpolation, 2025.
[13] Rishabh Bhardwaj, Do Duc Anh, and Soujanya Poria. Language models are homer simpson!
safety re-alignment of fine-tuned language models through task arithmetic, 2024.
[14] Le Yu, Bowen Yu, Haiyang Yu, Fei Huang, and Yongbin Li. Language models are super mario:
Absorbing abilities from homologous models as a free lunch, 2024.
[15] Prateek Yadav, Derek Tam, Leshem Choshen, Colin Raffel, and Mohit Bansal. Ties-merging:
Resolving interference when merging models, 2023.
11
==End of OCR for page 11==

==Start of OCR for page 12==
[16] Enneng Yang, Zhenyi Wang, Li Shen, Shiwei Liu, Guibing Guo, Xingwei Wang, and Dacheng
Tao. Adamerging: Adaptive model merging for multi-task learning, 2024.
[17] Zhichao Wang, Bin Bi, Shiva Kumar Pentyala, Kiran Ramnath, Sougata Chaudhuri, Shubham
Mehrotra, Zixu, Zhu, Xiang-Bo Mao, Sitaram Asur, Na, and Cheng. A comprehensive survey
of llm alignment techniques: Rlhf, rlaif, ppo, dpo and more, 2024.
[18] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin,
Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton,
Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano,
Jan Leike, and Ryan Lowe. Training language models to follow instructions with human
feedback, 2022.
[19] Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and
Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model,
2024.
[20] Harrison Lee, Samrat Phatale, Hassan Mansoor, Thomas Mesnard, Johan Ferret, Kellie Lu,
Colton Bishop, Ethan Hall, Victor Carbune, Abhinav Rastogi, and Sushant Prakash. Rlaif vs.
rlhf: Scaling reinforcement learning from human feedback with ai feedback, 2024.
[21] Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu,
Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu,
Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu,
Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji
Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yan Fang, Yang
Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5
technical report, 2025.
[22] MAA. American invitational mathematics examination - aime. In American Invitational
Mathematics Examination - AIME 2024, February 2024.
[23] Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li,
Chenjun Xiao, Chenzhuang Du, Chonghua Liao, Chuning Tang, Congcong Wang, Dehao Zhang,
Enming Yuan, Enzhe Lu, Fengxiang Tang, Flood Sung, Guangda Wei, Guokun Lai, Haiqing
Guo, Han Zhu, Hao Ding, Hao Hu, Hao Yang, Hao Zhang, Haotian Yao, Haotian Zhao, Haoyu
Lu, Haoze Li, Haozhen Yu, Hongcheng Gao, Huabin Zheng, Huan Yuan, Jia Chen, Jianhang
Guo, Jianlin Su, Jianzhou Wang, Jie Zhao, Jin Zhang, Jingyuan Liu, Junjie Yan, Junyan Wu,
Lidong Shi, Ling Ye, Longhui Yu, Mengnan Dong, Neo Zhang, Ningchen Ma, Qiwei Pan,
Qucheng Gong, Shaowei Liu, Shengling Ma, Shupeng Wei, Sihan Cao, Siying Huang, Tao
Jiang, Weihao Gao, Weimin Xiong, Weiran He, Weixiao Huang, Wenhao Wu, Wenyang He,
Xianghui Wei, Xianqing Jia, Xingzhe Wu, Xinran Xu, Xinxing Zu, Xinyu Zhou, Xuehai Pan,
Y. Charles, Yang Li, Yangyang Hu, Yangyang Liu, Yanru Chen, Yejie Wang, Yibo Liu, Yidao
Qin, Yifeng Liu, Ying Yang, Yiping Bao, Yulun Du, Yuxin Wu, Yuzhi Wang, Zaida Zhou,
Zhaoji Wang, Zhaowei Li, Zhen Zhu, Zheng Zhang, Zhexu Wang, Zhilin Yang, Zhiqi Huang,
Zihao Huang, Ziyao Xu, and Zonghan Yang. Kimi k1.5: Scaling reinforcement learning with
Ilms, 2025.
[24] Yuyan Zhou, Liang Song, Bingning Wang, and Weipeng Chen. Metagpt: Merging large
language models using model exclusive task arithmetic, 2024.
[25] Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi,
Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. s1: Simple
test-time scaling, 2025.
[26] Liang Wen, Yunke Cai, Fenrui Xiao, Xin He, Qi An, Zhenyu Duan, Yimin Du, Junchen Liu,
Lifu Tang, Xiaowei Lv, Haosheng Zou, Yongchao Deng, Shousheng Jia, and Xiangzheng Zhang.
Light-r1: Curriculum sft, dpo and rl for long cot from scratch and beyond, 2025.
[27] Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi
Hu, Xu Han, Yujie Huang, Yuxiang Zhang, Jie Liu, Lei Qi, Zhiyuan Liu, and Maosong Sun.
Olympiadbench: A challenging benchmark for promoting agi with olympiad-level bilingual
multimodal scientific problems, 2024.
12
==End of OCR for page 12==

==Start of OCR for page 13==
[28] Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay
Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam
Neyshabur, Guy Gur-Ari, and Vedant Misra. Solving quantitative reasoning problems with
language models, 2022.
13
==End of OCR for page 13==

==Start of OCR for page 14==
A Training Details
For both models, we selected 2,500 problems from the MixMathematics as training data. For each
problem, we sample 12 times. From each set of solutions, we randomly selected 2 solutions for
training. After computing the rewards, we normalized the reward values. Both models are trained
with 8 * A800-80G GPUs. The other hyperparameters used in the training process are presented in
the table below.
Table 4: Hyperparameters for the Deepseek-Distill-1.5B and Deepseek-Distill-7B.
Hyperparameter Deepseek-Distill-1.5B
Deepseek-Distill-7B.
cutoff_len
4096
4096
batch_size
32
32
learning_rate
5.0e-7
5.0e-7
num_train_epochs
2.0
2.0
lr_scheduler_type
constant
constant
M1
4
4
M2
beta
2
2
0.05
0.1
B Further Evalution of Different Methods
We further evaluate the performance and efficiency of different methods (AdaR1, DPO, O1-Pruner)
across varying levels of problem difficulty, as illustrated in Figure 5 and Figure 6. Figure Figure 5
presents the accuracy ratio of each method relative to a baseline model across different difficulty
levels within the MATH dataset. The results indicate that while performance trends may vary, our
proposed AdaR1 method demonstrates strong robustness. Specifically, as the inherent difficulty of
the mathematical problems increases, AdaR1 is able to consistently maintain a high accuracy ratio.
Figure 6 show the ratio of average tokens consumed by each method to solve problems across the
same difficulty spectrum. As expected, solving more difficult problems generally requires more
reasoning steps and thus more tokens. However, Figure 2 reveals that AdaR1 exhibits favorable
token efficiency. Critically, when faced with increasing problem difficulty, AdaR1 manages to solve
these complex problems while utilizing relatively fewer tokens compared to other evaluated methods,
showcasing its ability to achieve efficient reasoning even for demanding tasks.
Collectively, these figures highlight AdaR1's ability to strike a beneficial balance between accuracy
and efficiency. It not only maintains high performance on challenging problems (Figure 5) but also
does so in a computationally efficient manner, particularly evident in its lower token usage for difficult
instances (Figure 6), addressing limitations observed in prior methods.
C Case Study
14
==End of OCR for page 14==

==Start of OCR for page 15==
Tokens
Accuracy
0.75
AdaR1
-DPO
01-Pruner
-
-Long
---Short
0.5
Level 1
Level 2
Level 3
Level 4
Level 5
Different Difficulty Levels
Figure 5: The ratio of accuracy at different MATH levels on different models. As the difficulty
increases, AdaR1 is able to maintain high accuracy.
- AdaR1
DPO
01-Pruner
---Long
--- Short
5400
4750
4100
3450
2800
2150
1500
850
200
Level 1
Level 2
Level 3
Level 4
Level 5
Different Difficulty Levels
Figure 6: The ratio of average tokens on different models. As the difficulty increases, AdaR1 is able
to use relatively fewer tokens to solve difficult problems.
15
==End of OCR for page 15==

==Start of OCR for page 16==
Question: Convert the point (0,3) in rectangular coordinates to polar coordinates.
Enter your answer in the form (r, 0), where r > 0 and 0 ≤ 0 < 2π. Let's think step by
step and output the final answer within boxed{}. Ground Truth: (3,1)
AdaR1: To convert the rectangular coordinate (0, 3) to polar coordinates, we use the formulas:
r = √x2 + y2 and 0 = arctan(2). For the point (0, 3): - r = √02 + 32 = √9 = 3 - Since
the point lies on the positive y-axis, the angle 0 is 7. Thus, the polar coordinates are (3,5).
Final Answer:
π
(3,5)
2
✓
R1: Okay, so I need to convert the rectangular coordinate (0, 3) to polar coordinates. Hmm,
polar coordinates are represented as (r, θ), where r is the radius (distance from the origin)
and @ is the angle made with the positive x-axis. Let me recall the formulas to convert from
rectangular to polar coordinates.
First, I remember that the radius r can be found using the Pythagorean theorem. Since in
rectangular coordinates, the point is (x, y), the distance from the origin is r = √x2 + y². So
in this case, x is 0 and y is 3. Plugging that into the formula, r should be √02 + 32 which is
√0 + 9 = √9 = 3. Okay, that seems straightforward.
Now, the angle θ. I know that @ is calculated using the arctangent function because tan 0 = .
But wait, in this case, x is 0. Hmm, arctangent of. But dividing by zero is undefined. I
need to think about what angle e corresponds to when x is 0.
Y
x
In rectangular coordinates, when x is 0, the point lies on the y-axis. Since y is positive
here (y = 3), the point is on the positive y-axis. So, in terms of polar coordinates, the
angle @ should be the angle that points directly upwards along the y-axis. I think that's
radians, which is 90 degrees. Because starting from the positive x-axis, if we rotate 17 radians
counterclockwise, we end up pointing straight up along the positive y-axis.
Let me double-check that. If 0 is, then converting back to rectangular coordinates, x should
ber cos 0 = 3 cos(17) and y should be r sin 0 = 3 sin().cos() is 0 and sin() is 1, so
that gives x = 0 and y = 3, which is exactly the point we started with. So that makes sense.
Wait, but could @ also be ? No, because 3 is pointing downward along the negative y-axis,
which would give us y = −3, not 3. So 0 has to be in the range where the point is on the
positive y-axis, which is .
Just to make sure, let's think about the unit circle. At 0 = 0, we're on the positive x-axis; at
0=, positive y-axis; at 0 = π, negative x-axis; at 0 3, negative y-axis. So yeah, 0 =
is correct for the positive y-axis.
So, putting it all together, r is 3 and 6 is . Therefore, the polar coordinates should be (3, 플).
Let me just write that out to make sure I didn't make any mistakes.
Wait, another thought: sometimes people use different ranges for θ, like from –π to π. But in
this case, the problem specifies that @ should be between 0 and 2ㅠ, so is within that range.
So, I don't need to adjust @ or anything.
Just to recap: the point is (0, 3) in rectangular, so x = 0, y = 3. Calculating r: √02 + 32 = 3.
Calculating 0: since x 0 and y is positive, is. So, (3, 1) is the polar coordinate.
I think that's solid. Let me visualize it too. If I plot (0,3) on the rectangular coordinate
system, it's on the y-axis, three units up. In polar coordinates, that's a radius of 3 and an
angle of from the positive x-axis. Yep, that lines up perfectly.
Just to add another check, let me compute the sine and cosine with 0 = . cos(1) is 0, so
x = 3 * 0 = 0, which matches. sin() is 1, so y = 3 * 1 = 3, which also matches. Perfect,
no discrepancies there.
So, I think I'm confident that the polar coordinates are (3, 1). I don't see any mistakes in my
reasoning, and all the checks seem to confirm that this is the correct answer.
π
Final Answer: The polar coordinates are (3,5)
Figure 7: AdaR1 can infer answers to simple problems with a small amount of CoT, whereas the R1
model requires a long CoT.
16
==End of OCR for page 16==

==Start of OCR for page 17==
Question: Define p
1
k=1 k2
100
=
1 and q
=
78
1
k=1 k3.
1. Find a way to write
j=1 k=1 (3+k)3 in terms of p and q. Let's think step by step and output the fi-
100
100
1
nal answer within boxed{}. Ground Truth: p − q
100
18
1
100 8
AdaR1: Okay, so I have this problem where I need to express a double sum in terms of two
known series, p and q. Let me first write down what's given. We have: p = ∑k=1 and
q = ∑k=1 . And the expression we need to evaluate is ∑;=1 k=1 (+6)3. Hmm, so
this is a double sum over j and k starting from 1 to infinity, and each term is 1 divided by
(j+k) cubed. I need to find a way to write this in terms of p and q. Let me think about
how to approach this. Maybe I can change variables to simplify the expression. If I let
n = j + k, then for each fixed n, j can range from 1 to n-1, and k would be n - j. So, for
each n, the number of terms where j + k = n is n - 1. So, maybe I can rewrite the double
sum as a single sum over n, and for each n, sum over j from 1 to n-1. Let me write that
down: j=1 k=1 (3+k)3 = 2n=2 j=1 3. Yes, that seems right. Because when j + k =
n, j can go from 1 to n-1, and for each j, k is determined as n - j. So, the inner sum is just
100
1
100
n-1
adding (n-1) times. So, the double sum becomes: 2 31. Simplify that expression:
Σ2 = 2(2-3). Yes, because 1 = - =
-. So now, the sum
78
n-1
18
n-1
n3
n3
1
100
can be split into two separate sums: ns: ∑n=272 - Σn=2. 5. But notice that these sums are just
p and q minus their first term. Let's write that: The first sum is: Σ=272 = p - → = p − 1.
Similarly, the second sum is: ∑=2 3 = q - 1 = q - 1. Therefore, the original
[OMITTED]
Thus, the original double sum becomes: (p − 1) − (q - 1) = p − 1-q+1 = p-q. Therefore,
the final answer is: p
q =
1
q
100
878
1
R1: Okay, so I have this problem where I need to express a double sum in terms of two
known series, p and q. Let me first write down what's given. We have: p = ∑k=12 and
k=1 3. And the expression we need to evaluate is ∑j=1 k=1 (j+k)3. Hmm, so this
is a double infinite sum where both j and k start from 1 and go to infinity, and each term is 1
over (j + k) cubed. I need to find a way to write this in terms of p and q. Let me think about
how to approach this. Maybe I can find a substitution or change variables to simplify the
expression. Since the denominator is (j + k)³, maybe if I let n = j + k, then I can rewrite the
sum in terms of n and another variable. Let's try that. Let me set n = j + k. Then, for each
fixed n, j can range from 1 to n - 1, because k would be n - j, which also needs to be at least 1.
So, for each n, j goes from 1 to n - 1, and k = n - j. So, substituting, the double sum becomes:
Σ=2 =1 3. Wait, that seems promising. Because now for each n, we have a term
n3
multiplied by the number of j's, which is (n - 1). So, the inner sum is just 3¹. Therefore,
the entire expression becomes: ∑n=231. Simplify that: ∑m=2(2-). Which is:
100
n-1 1
78
1
10∞
81
100
1
q
n
n
1
Σ2-23. But I know that: 12 = p and
n=1 1 n³ = q. So, subtracting
the first term from both series: ∑=2
= p-1 1 and
2
1. Therefore,
substituting back into the expression: (p − 1) – (q − 1) = p − 1 − q + 1 = p - q. Wait, so
the entire double sum simplifies to p - q? Let me verify my steps to make sure I didn't make a
mistake. 1. I started by changing variables, setting n = j + k. Then for each n, j goes from 1 to
n - 1, so k = n - j also goes from 1 to n - 1. That seems correct. 2. Then, I rewrote the double
sum as a single sum over n, with each term being 1. That makes sense because for each n,
there are (n - 1) terms contributing 곯 each. 3. Then, I split the fraction into - . That's
a valid step because = - = 글 - . 4. Then, I separated the sums into two
infinite series starting from n = 2, which is correct because the original sum starts from n = 2.
5. Finally, I expressed the sums in terms of p and q by subtracting the n=1 term from each
series. That's correct because the original p and q start from n=1. So, putting it all together,
the double sum equals p - q. That seems neat.But wait, let me double-check. Maybe I can
compute the original double sum numerically for a few terms and see if it approximates to p -
q. Let me compute p and q first. We know that...
[OMITTED]
n-
Therefore, the final answer is: p-q
1
3
n
n3
Figure 8: AdaR1 can infer answers to difficult problems with relatively few long CoT, whereas the
R1 model requires longer CoT.
17
==End of OCR for page 17==