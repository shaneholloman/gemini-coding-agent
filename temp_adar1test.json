{
  "conversation_history": [
    "parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text='CONTEXT FROM PDF (\\'AdaR1-FromLong-CoTtoHybrid-CoTviaBi-LevelAdaptiveReasoningOptimization2504.21659v1.pdf\\', ID: 4):\\n---\\nHere is the extracted text content from the provided PDF document:\\n\\nAdaR1: From Long-CoT to Hybrid-CoT via\\nBi-Level Adaptive Reasoning Optimization\\n\\nHaotian Luo\u00b9, Haiying He2* Yibo Wang\u00b3, Jinluan Yang\u2074, Rui Liu\u2075\\nNaiqiang Tan\u2075, Xiaochun Cao\u00b9, Dacheng Tao\u2075, Li Shen\u00b9\u2020\\n\u00b9 Sun Yat-sen University; \u00b2 China Agricultural University; \u00b3 Tsinghua University;\\n\u2074 Zhejiang University; \u2075 Didichuxing Co. Ltd; \u2076 Nanyang Technological University\\n\\nAbstract\\nRecently, long-thought reasoning models achieve strong performance on complex\\nreasoning tasks, but often incur substantial inference overhead, making efficiency a\\ncritical concern. Our empirical analysis reveals that the benefit of using Long-CoT\\nvaries across problems: while some problems require elaborate reasoning, others\\nshow no improvement or even degraded accuracy. This motivates adaptive reason-\\ning strategies that tailor reasoning depth to the input. However, prior work primarily\\nreduces redundancy within long reasoning paths, limiting exploration of more ef-\\nficient strategies beyond the Long-CoT paradigm. To address this, we propose a\\nnovel two-stage framework for adaptive and efficient reasoning. First, we construct\\na hybrid reasoning model by merging long and short CoT models to enable diverse\\nreasoning styles. Second, we apply bi-level preference training to guide the model\\nto select suitable reasoning styles (group-level), and prefer concise and correct\\nreasoning within each style group (instance-level). Experiments demonstrate that\\nour method significantly reduces inference costs compared to other baseline ap-\\nproaches, while maintaining performance. Notably, on five mathematical datasets,\\nthe average length of reasoning is reduced by more than 50%, highlighting the\\npotential of adaptive strategies to optimize reasoning efficiency in large language\\nmodels. Our code is coming soon at https://github.com/StarDewXXX/AdaR1\\n\\n1 Introduction\\nRecent advancements in large language models (LLMs), such as OpenAI\\'s O1[1] and Deepseek\\'s\\nR1[2], feature the adoption of detailed and complex reasoning processes (Long-CoT) analogous\\nto human deliberation on complex problems. This reasoning paradigm demonstrably enhances the\\nmodels\\' problem-solving capabilities and yields promising results. However, the enhanced reasoning\\ncapability derived from Long-CoT often comes at substantial inference overhead. Generating lengthy\\nsequences of intermediate reasoning steps incurs significant computational costs, increases latency,\\nand consumes considerable resources[3-5]. This efficiency bottleneck poses a critical challenge\\nfor deploying these powerful reasoning models in real-world, resource-constrained applications or\\ninteractive scenarios where responsiveness is paramount.\\n\\nExisting approaches to improving reasoning efficiency often focus on optimizing within the Long-\\nCoT distribution. Techniques such as O1-Pruner [6], Training and Overthinking compress reasoning\\npaths with various methods but they are inherently limited because they still choose long-thought\\ndistribution as the starting point. They primarily address redundancy within a potentially unnecessarily\\nlong path, rather than focusing whether such a long path was needed in the first place. This leaves\\n\\n*Equal contribution\\nCorresponding Author: Li Shen (shenli6@mail.sysu.edu.cn)\\nPreprint. Under review.\\n\\nunexplored the potential efficiency gains from fundamentally choosing a different, more concise\\nreasoning strategy altogether when appropriate. CoT-Valve[7] allows models to produce both extended\\nand concise responses simultaneously; however, it lacks mechanisms for adaptively selecting the\\noptimal reasoning path contingent on the characteristics of the input problem, which brings degraded\\nperformance.\\n\\nOur investigation (presented in Section 3) about the benefit of Long-CoT reasoning reveals a crucial\\ninsight: the utility of long, elaborate reasoning chains is highly problem-dependent. While complex\\nproblems genuinely benefit from detailed, step-by-step derivations, many other problems can be\\nsolved accurately and more efficiently with shorter, more direct reasoning paths. In fact, for simpler\\nproblems, forcing a Long-CoT process might not only be wasteful but can sometimes even introduce\\nerrors or degrade performance. This observation strongly motivates the need for adaptive reasoning\\nstrategies \u2013 systems that can tailor the depth and style of their reasoning process to the specific\\ndemands of the input problem.\\n\\nInspired by this, we propose a novel two-stage framework designed to enable efficient and adaptive\\nreasoning. Our core idea is to move beyond optimizing a single reasoning style and instead equip the\\nmodel with the flexibility to choose between different reasoning approaches. First, we construct a\\nhybrid reasoning model by integrating the capabilities of both Long-CoT and Short-CoT models. This\\nhybrid model serves as a reasoning model capable of generating both reasoning styles. Second, we\\nintroduce a specialized training methodology called Bi-Level Adaptive Reasoning Optimization. This\\ntraining process operates on two levels: (i) Group-Level Preference: It guides the hybrid model to\\nimplicitly discern the overall complexity of a given problem and select the most appropriate reasoning\\nstyle (e.g., Long-CoT for complex tasks, Short-CoT for simpler ones). (ii) Instance-Level Preference:\\nWithin the chosen style group (long or short), it further refines the model\\'s output by encouraging it\\nto prefer reasoning paths that are not only correct but also concise.\\n\\nBy training the model to make intelligent choices at both the style-selection level and the instance-\\ngeneration level, our framework aims to dynamically allocate computational resources where they are\\nmost needed. As demonstrated through our experiments, this approach leads to significant reductions\\nin inference costs compared to standard Long-CoT models and other efficiency-focused baselines,\\nwhile maintaining strong reasoning performance. Specifically, in MATH[8], the reasoning length\\ndecreases by 58% while maintaining accuracy, while in GSM8K[9], the length decreases by 74%\\nalongside an improvement in accuracy. This work underscores the promise of adaptive strategies as a\\nkey direction for optimizing the trade-off between reasoning quality and computational efficiency of\\nlarge reasoning models.\\n\\nOur contributions can be summarized as follows:\\n\\n\u2022 We conduct an empirical analysis investigating the benefits of long Chain-of-Thought\\n(CoT) reasoning relative to shorter CoT approaches, identifying the conditions under which\\nextended reasoning paths offer tangible advantages.\\n\u2022 We propose using Adaptive Hybrid Reasoning Model to enhance inference efficiency,\\naccompanied by a novel training pipeline (AdaR1). Comprehensive experiments demonstrate\\nthat our proposed method achieves excellent performance, significantly improving efficiency\\nwhile maintaining high accuracy.\\n\u2022 We perform further analyses on the resulting Adaptive Hybrid Reasoning Model to gain\\ndeeper insights into its characteristics and operational behavior. And we will release the\\nmodel weights of the Adaptive Hybrid Reasoning Model to the public to encourage further\\nresearch and application by the community.\\n\\n2 Related Work\\n2.1 Model Merging\\n\\nModel merging[10] is an emerging technique that combines the parameters of multiple trained\\nmodels into a single one without requiring access to the original training data. Unlike ensemble\\nlearning[11], which aggregates outputs at inference time, model merging operates at the parameter\\nlevel, resulting in a more efficient and compact model. Recent researchers have proposed various\\nmerging methods, including parameter interpolation[12] and alignment-based strategies[13], and\\n2\\n\\nexplored their applications in large language models, multimodal language models and many subfields\\nin machine learning. Beyond simple linear merging, which averages the model parameters, researchers\\nhave proposed more advanced methods such as DARE[14], TIES-Merging[15], and AdaMerging[16].\\nDARE reduces delta parameter redundancy by dropping and rescaling parameters before merging.\\nTIES-Merging focus on reducing interference from redundant parameters and sign disagreements by\\ntrimming, electing parameter signs, and performing disjoint merges. AdaMerging adaptively adjusts\\ntask or layer weights via entropy minimization on unlabeled test samples to enhance merged model\\nperformance without original training data.\\n\\nDifferent from traditional model merging, which integrates parameters from multiple source models\\nto consolidate learned capabilities, our work try to empower a single model to adaptively select an\\nappropriate reasoning mode (Long-CoT or Short-CoT) for each problem instance, primarily targeting\\nthe optimization of computational efficiency in reasoning rather than multi-task performance.\\n\\n2.2 Alignment\\n\\nLLM alignment[17] research aims to ensure that large language models generate responses that\\nmeet human expectations, enabling us to utilize these powerful language models in a reliable\\nand responsible way. Reinforcement Learning from Human Feedback (RLHF)[18] is one of the\\nmainstream methods, which optimizes the model based on human feedback, but its high computational\\ncost has led researchers to explore more efficient alignment techniques, such as Direct Preference\\nOptimization (DPO)[19], which simplify the alignment process by directly optimizing the LLM\\npolicy based on preference data. other approaches such as Reinforcement Learning from AI Feedback\\n(RLAIF)[20] utilize AI-generated feedback to reduce costs and enhance alignment efficiency. These\\ndiverse techniques reflect the dynamic evolution of the field, with the aim of improving both the\\neffectiveness and efficiency of LLM alignment.\\n\\nAlthough we use preference learning, our method diverges from the objectives of LLM alignment.\\nWhile alignment techniques focus on shaping model outputs and behavior to conform with human\\npreferences, values, and safety criteria, our research optimizes the internal reasoning generation\\nprocess-specifically, the Chain-of-Thought. The aim is to enhance problem-solving efficiency and\\naccuracy rather than aligning model behavior with human judgment.\\n\\n3 Motivation\\n3.1 Problem Setup\\n\\nChain-of-Thought (CoT) prompting has emerged as a powerful technique for enhancing the reasoning\\ncapabilities of large language models. Within the CoT paradigm, a distinction can be made between\\nLong-CoT, which involves generating detailed and extensive thinking steps, and Short-CoT, which\\ndirectly generate solving steps.\\n\\n3.2 When Do We Need Long-CoT?\\n\\nSimply applying Long-CoT uniformly across all problems leads to unnecessary overhead on instances\\nwhere its detailed steps are not required or do not yield accuracy gains. Therefore, we empirically\\ninvestigate the necessity of Long-CoT and analyze the relationship between problem characteristics\\nand Long-CoT efficacy in this subsection. We design an experiment to analyze the accuracy gains of\\nLong-CoT over Short-CoT across different types of problems. Specifically, we use DeepSeek-R1-\\nDistill-Qwen-7B as the Long-CoT model. For the Short-CoT model, we fine-tune the DeepSeek-R1-\\nDistill-Qwen-7B using 2,000 Short-CoT samples generated by Qwen2.5-Math-7B-Instruct[21]. We\\ndo not directly use Qwen2.5-Math-7B-Instruct as the Short-CoT model, since its training template\\ndiffers substantially from that of the DeepSeek-Distill series, which could introduce unpredictable\\neffect in the subsequent model merging and sampling process. We construct a mixed dataset (called\\nMixMathematics) which comprises training examples from AIME[22], MATH, and GSM8K. Further\\ndatasets details can be found in the Section 5.1.\\n\\nWe selected 2,500 samples and generated 12 responses for each question. To refine our analysis\\nand focus on solvable instances, we first removed samples where both the long and short CoT\\naccuracies were zero, as such cases typically indicate problems exceeding the model\\'s current\\n3\\n\\nFigure 1: The proportion of gain in the data (left) and the relationship between CoT length and\\naccuracy improvement (right), Long-CoT reasoning improves accuracy on difficult problems but has\\nlittle effect or harms performance on easy ones.\\n\\nMethod Optimization Scope Performance (Accuracy)\\nOverthinking[4] Limited x Slightly Dropped \u2713\\nkimi-1.5[23] Limited x Slightly Dropped \u2713\\nO1-Pruner Limited x Not Dropped \u2713\\nNaive Merge Broad\u2713 (mostly) Dropped \u00d7\\nCoT-Valve Broad\u2713 Dropped \u00d7\\nAdaR1(Ours) Broad\u2713 Slightly Dropped \u2713\\nTable 1: Comparison of Different Methods. The term \"limited\" refers to an optimization scope\\nwhere the model operates within the distribution of long-CoT, inherently limiting its capacity for\\nefficiency. In contrast, \"broader\" denotes an optimization approach that encompasses both long-CoT\\nand short-CoT, allowing the model to achieve efficiency by generating shorter responses without\\nthinking. \"slightly dropped\" indicates a reduction in accuracy of less than 3%. Conversely, the\\ndesignation \"dropped\" signifies a more substantial performance degradation, corresponding to a\\ndecrease in accuracy exceeding 3%.\\n\\ncapabilities. Subsequently, we calculated the accuracy for both the long and short CoT samples and\\nthen determined the accuracy gain (i.e., the accuracy of the Long-CoT minus the accuracy of the\\nShort-CoT). Our analysis proceeded in two phases. First, as depicted in the left portion of Figure 1,\\nwe quantified the proportion of samples falling into three categories based on the accuracy gain: gain\\n> 0, gain = 0, and gain < 0. Notably, we observed that for nearly half of the samples, employing a\\nLong-CoT did not yield any improvement, and in a small fraction of cases, it even led to a decrease\\nin performance. To further investigate this phenomenon, we categorized the samples based on the\\naverage length of their Long-CoT reasoning, as length is often correlated with problem difficulty. The\\nright portion of Figure 1 reveals that the benefit derived from Long-CoT becomes more pronounced\\nfor problems with greater length (and thus, presumably higher difficulty). In the shortest length\\ninterval, Long-CoT offered negligible gains, whereas in the longest length interval, it resulted in a\\nsubstantial improvement in accuracy.\\n\\nPrior methods (Table 1), such as Overthinking [4], kimi-1.5 [23], and O1-Pruner, typically operate\\nwithin a limited optimization scope but generally maintain performance stability or incur only a slight\\ndrop, with O1-Pruner notably achieving no performance decrease. In contrast, methods designed\\nfor a broad optimization scope, including Model Merge and CoT-Valve, did not consider how to\\ntackle easy and different problems, rendering the model incapable of determining its reasoning depth\\naccording to the inherent difficulty of the task. Thus they frequently result in significant performance\\ndegradation. In a nutshell, methods with a restricted optimization can generally preserve performance\\nbut lose the chance to utilize shorter CoT. However, approaches capable of utilize broader CoT\\ndistribution have struggled to maintain accuracy due to their inability to adapt adequate reasoning\\ndepth to problem complexity.\\n\\nThe finding mentioned in last section motivates us to address the efficiency challenge of Long-CoT\\nmodels from a novel perspective: enabling the reasoning model to adaptively select an appropriate\\n4\\n\\nreasoning mode (long or short CoT) for different problems, and then generate a correct and concise\\nCoT in the determined mode. Our proposed method (AdaR1) differentiates itself by successfully\\nachieving a broad optimization scope while incurring only a marginal performance decrement. This\\ndemonstrates a more favorable trade-off between efficiency and accuracy compared to existing\\nbroad-scope optimization techniques.\\n\\n4 Bi-Level Adaptive Reasoning Optimization\\n4.1 Problem Setup\\n\\nWe consider a LLM parameterized by \u03b8 and denoted as \u03c0\u03b8. In the context of math problem solving,\\nthe LLM accepts a sequence x = [x\u00b9, . . ., xn], commonly termed as the problem, and then generate\\na corresponding solution y = [y\u00b9, . . ., ym]. Hence, the solution y is construed as a sample drawn\\nfrom the conditional probability distribution \u03c0\u03b8(\u00b7|x). The conditional probability distribution \u03c0\u03b8(y|x)\\ncan be decomposed as follows:\\nm\\n\u03c0\u03b8(y|x) = \u03a0 \u03c0\u03b8(yi|x, yi<i).\\nj=1 (1)\\n\\nWe consider two LLMs: one trained to generate long, reflective Chain-of-Thought (CoT) reasoning\\n(Long-CoT model, denoted as \u03b8L) and the other trained for short and concise reasoning paths (Short-\\nCoT model, denoted as \u03b8S). These two models are typically fine-tuned with different CoT and\\ndemonstrate distinct reasoning patterns.\\n\\n4.2 Method Overview\\n\\nOur method consists of two stages, shown in Figure 2. First, we merge a Long-CoT model and a\\nShort-CoT model to obtain a unified reasoning model capable of generating both types of reasoning\\npaths. This allows exploration over a broader CoT distribution. In the second stage, we apply\\nBi-Level Preference Training: for group-level preference, the model learns to choose between long\\nand short reasoning group based on the input; for instance-level preference, it learns to compress the\\nreasoning path to improve efficiency within the chosen group determined by group-level preference.\\n\\nFigure 2: Pipeline of AdaR1. At Stage I, we fused the models to obtain \u03c0\u03b8H. In Stage II, we sample\\nfrom both long and short models and then elicit the group-level and instance-level preference. After\\nthis, we optimize \u03c0\u03b8H at both group and instance level to obtain a hybrid adaptive reasoning model.\\n\\n4.3 Stage I: Long-and-Short Reasoning Merge\\n\\nTo enable flexible reasoning behaviors within a single model, we first perform model merging with\\nlong and short models. We adopt a simple yet effective strategy of linearly merging their parameters.\\nGiven two models with parameters \u03b8L and \u03b8S, we compute the merged model as:\\n\u03b8H = \u03b1\u03b8L + (1 \u2212 \u03b1)\u03b8S,\\n(2)\\n5\\n\\nwhere a \u2208 [0,1] is a merging coefficient that balances the contribution from each model. The\\nresulting hybrid reasoning model, \u03c0\u03b8H, inherits the capacity to generate both long and short CoT\\ndepending on the input.\\n\\nThis merged model expands the diversity of the CoT distribution it can produce, laying the foundation\\nfor adaptive reasoning. By combining the strengths of both reasoning styles, it enables the model\\nto potentially match different problem types with suitable reasoning strategies, which is key to\\nimproving efficiency in the next stage.\\n\\n4.4 Stage II: Bi-Level Preference Training\\n\\nIn this stage, we introduce a Bi-Level Preference Training strategy to fine-tune the model toward\\nefficient reasoning. The core idea is to train the model to: (1) select the appropriate reasoning style\\n(long or short) for each problem (group-level preference) and (2) further compress the reasoning\\nwithin the determined chosen group (instance-level preference).\\n\\nGroup Labels. We define a group label g to denote the reasoning style of a response group. Let\\ngL denote the long reasoning group and gS denote the short reasoning group. For a given input\\nproblem x, a generated resposne (solution) y belongs to one of the two groups. We use {yi}g=gL to\\ndenote the set of K Long-CoT responses generated by the Long-CoT model \u03b8L, and {yj}g=gS for\\nthe corresponding short responses from the Short-CoT model \u03b8S.\\n\\nGroup-Level Preference. For each math problem x in the dataset D, we sample K solutions from\\nboth the long and short reasoning models. Let {yi}K i=1, and {yj}K j=1 be the respective sample sets.\\nWe define the approximated accuracy expectation for each group as:\\n\u00ca[CL(x)] =\\n1 K\\nK \u03a3 1[Correct(yi)], \u00ca[CS(x)] =\\ni=1\\n1 K\\nK \u03a3 1[Correct(yj)],\\nj=1 (3)\\n\\nwhere 1[.] is the indicator function. Then we introduce a preference margin threshold \u03b5 > 0. The\\ngroup-level preference for x is then determined as:\\ngL > gS x if \u00ca[CL(x)] \u2013 \u00ca[CS(x)] > \u03b5,\\ngS > gL x if \u00ca[CL(x)] \u2013 \u00ca[CS(x)] \u2264 \u03b5.\\n\\nGiven the group-level preference for an input x, we form training pairs from the Cartesian product of\\nthe two groups. For example, if gL > gS | x, we construct the preference pairs as:\\nPgroup(x) = {(yi, yj) | i \u2208 [1, K], j \u2208 [1, K]} .\\n(4)\\nFrom this set of pairs, we randomly sample a subset contain M\u2081 pairs to construct DPO training\\ntuples (x, yw, yl), where yw is the preferred (chosen) response and yl is the less preferred (rejected).\\nFor all x \u2208 D, we perform group-level preference assignment by comparing the sampled long and\\nshort responses as described above. These tuples are then aggregated into a new dataset Dgroup =\\n{(x, yw, yl)}, which serves as supervision for optimizing the DPO objective at the group level.\\n\\nInstance-Level Preference. Once the preferred group g* \u2208 {gL, gS} is determined for a given x,\\nwe further construct instance-level preferences within that group to encourage more concise reasoning.\\nWe compare response pairs (ya, y\u266d) such that both belong to the same group (e.g., ya, y\u266d \u2208 {yi}g=g*),\\nand prefer the shortest correct response. For dispreferred samples, we select M2 longest responses.\\nFormally, for each x \u2208 D with preferred group g*, we first identify the subset of correct responses\\n{yi}correct {yi}g=g*. Among these, we select the shortest correct response as the preferred instance:\\nyw = arg min |y|.\\ny\u2208{yi}correct\\n\\nTo construct instance-level preference pairs, we then select the M2 longest responses from the entire\\ngroup {yi}g=g*. Denote these as {yilj}M2 j=1. This yields a dataset of instance-level training tuples:\\nDinstance = {(x, yw, yl) | yw arg miny\u2208{yi}correct |y|, yl \u2208 arg max y\u2208{yi}g=g* |y|}\\n6\\n\\nBench\\nModel AIME25 MATH500 GSM8K Olympiad Minerva Avg.(%)\\n7B Models\\nLong(R1-distill) 38.3 90.2 88.9 54.4 35.7 -\\n(11005) (3534) (1014) (7492) (4533)\\nShort 10.0 78.6 89.5 39.4 28.6 -19.97%\\n(957) (591) (272) (910) (579) (-84.57%)\\nMerge 21.7 79.4 88.4 41.2 25.7 -18.63%\\n(9079) (916) (236) (3743) (1734) (-56.02%)\\nDPO 35.8 89.4 86.0 55.2 35.6 -3.56%\\n(9976) (2334) (360) (5309) (3281) (-33.26%)\\nO1-Pruner 40.0 92.4 89.4 55.3 35.3 +2.48%\\n(9353) (2212) (377) (5295) (3259) (-34.53%)\\nCoT-Valve 22.5 78.6 87.9 39.6 29.4 -18.41%\\n(5024) (747) (235) (2313) (629) (-73.06%)\\nAdaR1(Ours) 35.8 90.2 90.3 52.4 34.1 -1.65%\\n(8426) (1468) (260) (4889) (1647) (-50.93%)\\n1.5B Models\\nLong(R1-distill) 23.3 81.0 80.9 41.6 26.1 -\\n(12307) (4416) (1481) (7687) (5789)\\nShort 9.0 69.4 78.2 30.7 22.4 -26.34%\\n(1098) (740) (269) (1373) (725) (-85.15%)\\nMerge 20.8 71.8 74.2 28.6 20.0 -10.12%\\n(9226) (1740) (251) (3767) (1399) (-59.10%)\\nDPO 20.8 81.4 74.8 42.8 24.3 -5.93%\\n(10224) (3055) (374) (6319) (3905) (-34.57%)\\nO1-Pruner 23.3 82.6 84.6 44.7 28.3 +2.18%\\n(9496) (2782) (726) (5658) (3964) (-33.75%)\\nCoT-Valve 14.2 69.6 76.3 28.7 19.5 -19.61%\\n(7744) (1299) (205) (3169) (867) (-67.52%)\\nAdaR1(Ours) 23.0 80.8 79.2 42.1 23.5 -1.21%\\n(9516) (2455) (341) (5802) (3021) (-43.28%)\\nTable 2: Accuracy (shown above) and length (shown below) of models and methods on different\\nbenchmarks. Avg represents the change in length and accuracy compared to the Long model (+ for\\nincrease, - for decrease).\\n\\nThese instance-level preferences encourage the model not only to reason correctly, but also to do so\\nconcisely within the preferred reasoning style.\\n\\nWe sample such intra-group pairs and use them as additional training data for DPO to encourage the\\nmodel to favor more concise reasoning within each group.\\n\\nObjective. Given collelcted preference datasets Dgroup and Dinstance sampled from p* which contains\\nN preference pairs (x, yw, yl). With a parameter \u03b2 controlling the deviation from the reference model\\n\u03a0ref, DPO optimize the model by:\\nmax E(x,yw,yl)~DgroupUDinstance [log\u03c3 (\u03b2 log \u03c0\u03b8H (yw | x) \u2212 \u03b2 log \u03c0\u03b8\u03b7 (yl | x))]\\n\u03c0\u03b8H \u03a0oref (yw | X) \u03a0oref (Yl | X)\\n7\\n\\n5 Experiments\\n5.1 Setup\\n\\nLong-CoT Models. The long thought models we chosen for our experiment are Deepseek-R1-Distill-\\nQwen-7B and Deepseek-R1-Distill-Qwen-1.5B, which have demonstrated excellent performance on\\nmost math problem-solving tasks. For both models, we utilize full-parameter fine-tuning.\\n\\nShort CoT Models. Since model merging requires Shot-CoT models, we face two issues with\\nexisting Shot-CoT models: (1) they often employ templates that differ from those used in Long-\\nCoT models; (2) they tend to exhibit substantial parameter deviations from the base model, which\\nintroduces instability during the merging process [16, 24]. To address these challenges, we fine-tune\\nthe Long-CoT models using a small number of short CoT examples to obtain the corresponding\\nShot-CoT models. This approach ensures consistency in template usage and maintains a closer\\nparameter proximity between the two models.\\n\\nDataset. Following s1[25] and Light-R1[26], we construct a mixed training dataset to ensure coverage\\nacross mathematical problems of varying difficulty levels. Specifically, we combine GSM8K, MATH,\\nand AIME datasets in a ratio of 1:3:1, resulting in a total of 2,500 diverse math problems.\\n\\nEvaluation. We use the GSM8K test set, the MATH test set, and AIME25 as in-distribution\\nevaluation data, while Olympiad[27] and Minerva[28] are employed as out-of-distribution test sets.\\nFor evaluation metrics, we consider both accuracy and sequence length. Additionally, we report the\\naverage accuracy degrade rate and the average length reduction rate across all test sets.\\n\\n5.2 Competitive Methods\\n\\nDPO. DPO are widely used baselines in reasoning optimization area. Follwoing the setting of [23, 4],\\nwe choose shortest sample as chosen samples and longest sample as rejected sample.\\n\\nCoT-Valve. CoT-Valve enables dynamic control of Chain-of-Thought length using a single model by\\nidentifying and leveraging a controllable direction in the model\\'s parameter space, reducing inference\\ncost by compressing reasoning paths based on task difficulty.\\n\\n01-Pruner. 01-Pruner is a method designed to reduce reasoning overhead while maintaining model\\naccuracy. It begins by establishing a baseline through pre-sampling, and then applies reinforcement\\nlearning-based finetuning to guide the model toward generating more concise reasoning under\\naccuracy-preserving constraints.\\n\\n5.3 Main Results\\n\\nWe can be seen from the Table 2 that: the Short and Merge models achieve the most significant\\nlength reduction compared to the Long Model. However, this efficiency gain is accompanied by a\\nnotable degradation in accuracy, exceeding 10 percentage points. Among the models that do not\\nsuffer significant accuracy degradation, our method achieves the best length reduction performance,\\nreaching 50.93% for the 7B model and 43.28% for the 1.5B model. Compared to DPO, our approach\\ndemonstrates both more substantial length reduction and significantly less accuracy degradation.\\nWhile O1-Pruner maintains high accuracy, its length reduction effect is considerably weaker than that\\nof our method.\\n\\n5.4 Ablation Study\\n\\nTo understand the contribution of each component of our proposed framework, we conduct an\\nablation study. We evaluate several variants of our model on a subset of the benchmarks (AIME25,\\nMATH500, GSM8K). Table 3 presents the results. As shown, the Merge model achieves a significant\\naverage length reduction of 56.10% compared to the baseline, but this comes at a substantial accuracy\\ndegradation of 12.83%.\\n\\nNext, we investigate the effect of Supervised Fine-Tuning (using the chosen sample in our group\\nlevel preference dataset) on the merged model. SFT helps recover a significant portion of the lost\\naccuracy, bringing the average degradation down to 3.82%. However, its average length reduction is\\nless pronounced (31.86%) compared to the Merge model without further training.\\n8\\n\\nBench\\nModel AIME25 MATH500 GSM8K Avg.(%)\\nLong(R1-distill) 38.3 90.2 88.9 -\\n(11005) (3534) (1014)\\nMerge 21.7 79.4 88.4 -12.83%\\n(9079) (916) (236) (-56.10%)\\nMerge + SFT 35.8 84.6 88.7 -3.82%\\n(11222) (2314) (375) (-31.86%)\\nMerge + group level 30.8 87.8 91.6 -3.31%\\n(9049) (1565) (359) (-46.03%)\\nMerge + bi level 35.8 90.2 90.3 -0.51%\\n(8426) (1468) (260) (-52.08%)\\nTable 3: Ablation study of each component on several benchmarks, showing that the Merge + bi-level\\nachieves the best trade-off, with a 52.08% average length reduction and a minimal 0.51% accuracy\\ndegradation compared to others.\\n\\nIntroducing the group-level preference training after merging (Merge + group level) yields better\\nresults than SFT. It achieves a higher average length reduction (46.03%) and a slightly better accuracy\\nrecovery, with only a 3.31% average degradation relative to the baseline. This indicates that training\\nthe model to select the appropriate reasoning style is effective in balancing efficiency and accuracy.\\n\\nFinally, the full proposed method, incorporating both group and instance-level preference training\\n(Merge + bi level), demonstrates the most favorable trade-off. It achieves an impressive average\\nlength reduction of 52.08% while recovering accuracy to within 0.51% of the baseline, significantly\\noutperforming all other ablated variants in terms of accuracy retention while maintaining substantial\\nefficiency gains. This result highlights the complementary benefits of the bi-level training approach:\\nthe group level guides the model towards suitable reasoning styles, and the instance level further\\nrefines the chosen style by favoring concise and correct responses, leading to a highly efficient and\\naccurate hybrid reasoning model.\\n\\n6 Evaluation\\nIn this section, we analyze the reasoning behavior of the obtained Hybrid Adaptive Reasoning Model\\n(AdaR1) to gain deeper insights into its characteristics.\\n\\n6.1 Thinking Ratio Study\\n\\nTo investigate the thinking characteristics of different models, we propose the \"Thinking Ratio\"\\nmetric. This metric is designed to detect whether a response constitutes a deep thinking (Long-CoT)\\nsample. Long-CoT responses typically include unique keywords (e.g., \\'wait\\', \\'recheck\\', \\'hold on\\').\\nBy detecting the presence of these keywords in a response, we can effectively determine if it is a deep\\nthinking sample. This detection method is more generalizable than relying solely on response length.\\nWe considered a subset of the models from the main experiments and collected their responses on\\nthe Math Testset. Using the method described above, we analyzed the proportion of deep thinking\\nsamples for each model. Furthermore, for each category (thinking/non-thinking samples), we also\\ncalculated their accuracy.\\n\\nThe results are shown in Figure 3. The baseline Long-CoT model predominantly employs deep\\nthinking (0.98), yielding high accuracy. In contrast, the Naive Merge model drastically shifts towards\\nnon-thinking responses (0.94) but suffers significant accuracy degradation on both thinking (0.68)\\nand non-thinking (0.81) paths. DPO shows a moderate shift to non-thinking (0.34) while preserving\\naccuracy. Our AdaR1 model achieves a more significant shift towards non-thinking (0.72) than\\nDPO, yet crucially maintains high accuracy for these dominant non-thinking responses (0.96), unlike\\nthe Naive Merge. This demonstrates AdaR1\\'s effective adaptation, utilizing efficient shorter paths\\nwithout substantial accuracy loss.\\n9\\n\\nFigure 3: The proportion and accuracy of thinking and non-thinking in different methods, AdaR1 can\\nachieve a good balance and accuracy between thinking and non-thinking.\\n\\n6.2 Adaptive Reasoning Study\\n\\nThis section evaluates the adaptive reasoning capability of AdaR1(7B) models on mathematical tasks\\nusing the MATH dataset. The problems within the MATH dataset are categorized into five distinct\\ndifficulty levels (Level 1 to Level 5), making it suitable for our study. We analyze the AdaR1 model\\'s\\nthinking ratio (mentioned in last subsection) and average correctness across these five difficulty levels.\\n\\nAs illustrated in the left part of Figure 4, the AdaR1 model demonstrates a clear trend in the utilization\\nof thinking (Long-CoT) based on problem difficulty. A clear trend is observed where the proportion\\nof responses exhibiting long-CoT significantly increases with task difficulty. Specifically, Level\\n1 tasks show a very low thinking ratio, which progressively increases through intermediate levels,\\nreaching its highest ratio for Level 5 tasks. This suggests that the AdaR1 model adaptively engages\\nin thinking more frequently when confronted with more complex problems.\\n\\nRegarding average correctness, as depicted in the right panel of Figure 4, the AdaR1 model generally\\nperforms well across most difficulty levels. Its accuracy is comparable to that expected from a \\'pure\\'\\nLong-CoT model (Deepseek-R1-Qwen-7B-Distill) and significantly surpasses that of Short-CoT\\nmodel, particularly across levels 3 to 5. This outcome aligns with our expectation presented in Section\\n3, suggesting that the model has learned to adaptively employ Long-CoT when beneficial, thereby\\nachieving a favorable trade-off between computational efficiency and performance.\\n\\nFigure 4: The ratio of thinking and non-thinking CoTs of AdaR1-7B on different MATH levels (left)\\nand the accuracy on different MATH levels of different models (right). As the difficulty increases,\\nAdaR1 is able to think more on harder problems and maintain higher accuracy.\\n10\\n\\n7 Conclusion\\nIn this paper, we demonstrate through empirical analysis that the benefits of Long-CoT reasoning\\nvary significantly depending on the problem. Motivated by this, we propose a novel two-stage\\ntraining framework for adaptive reasoning. Experiments show that model trained with our method can\\nreason adaptively to different problems. And our method significantly reduces inference costs while\\npreserving performance, highlighting the promise of adaptive strategies for optimizing reasoning\\nefficiency in large language models.\\n\\nReferences\\n[1] OpenAI. Learning to reason with Ilms. https://openai.com/index/\\nlearning-to-reason-with-llms/, 2024. [Accessed 19-09-2024].\\n[2] DeepSeek-AI team. Deepseek-r1: Incentivizing reasoning capability in Ilms via reinforcement\\nlearning, 2025.\\n[3] Jeffrey Cheng and Benjamin Van Durme. Compressed chain of thought: Efficient reasoning\\nthrough dense representations, 2024.\\n[4] Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi\\nLiu, Mengfei Zhou, Zhuosheng Zhang, Rui Wang, Zhaopeng Tu, Haitao Mi, and Dong Yu. Do\\nnot think that much for 2+3=? on the overthinking of o1-like llms, 2025.\\n[5] Yang Sui, Yu-Neng Chuang, Guanchu Wang, Jiamu Zhang, Tianyi Zhang, Jiayi Yuan, Hongyi\\nLiu, Andrew Wen, Shaochen Zhong, Hanjie Chen, and Xia Hu. Stop overthinking: A survey on\\nefficient reasoning for large language models, 2025.\\n[6] Haotian Luo, Li Shen, Haiying He, Yibo Wang, Shiwei Liu, Wei Li, Naiqiang Tan, Xiaochun\\nCao, and Dacheng Tao. Ol-pruner: Length-harmonizing fine-tuning for o1-like reasoning\\npruning, 2025.\\n[7] Xinyin Ma, Guangnian Wan, Runpeng Yu, Gongfan Fang, and Xinchao Wang. Cot-valve:\\nLength-compressible chain-of-thought tuning, 2025.\\n[8] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn\\nSong, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset.\\nNeurIPS, 2021.\\n[9] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,\\nMatthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John\\nSchulman. Training verifiers to solve math word problems, 2021.\\n[10] Enneng Yang, Li Shen, Guibing Guo, Xingwei Wang, Xiaochun Cao, Jie Zhang, and Dacheng\\nTao. Model merging in llms, mllms, and beyond: Methods, theories, applications and opportu-\\nnities, 2024.\\n[11] Yichong Huang, Xiaocheng Feng, Baohang Li, Yang Xiang, Hui Wang, Bing Qin, and Ting Liu.\\nEnsemble learning for heterogeneous large language models with deep parallel collaboration,\\n2024.\\n[12] Yue Zhou, Yi Chang, and Yuan Wu. Mixup model merge: Enhancing model merging perfor-\\nmance through randomized linear interpolation, 2025.\\n[13] Rishabh Bhardwaj, Do Duc Anh, and Soujanya Poria. Language models are homer simpson!\\nsafety re-alignment of fine-tuned language models through task arithmetic, 2024.\\n[14] Le Yu, Bowen Yu, Haiyang Yu, Fei Huang, and Yongbin Li. Language models are super mario:\\nAbsorbing abilities from homologous models as a free lunch, 2024.\\n[15] Prateek Yadav, Derek Tam, Leshem Choshen, Colin Raffel, and Mohit Bansal. Ties-merging:\\nResolving interference when merging models, 2023.\\n11\\n\\n[16] Enneng Yang, Zhenyi Wang, Li Shen, Shiwei Liu, Guibing Guo, Xingwei Wang, and Dacheng\\nTao. Adamerging: Adaptive model merging for multi-task learning, 2024.\\n[17] Zhichao Wang, Bin Bi, Shiva Kumar Pentyala, Kiran Ramnath, Sougata Chaudhuri, Shubham\\nMehrotra, Zixu, Zhu, Xiang-Bo Mao, Sitaram Asur, Na, and Cheng. A comprehensive survey\\nof llm alignment techniques: Rlhf, rlaif, ppo, dpo and more, 2024.\\n[18] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin,\\nChong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton,\\nFraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano,\\nJan Leike, and Ryan Lowe. Training language models to follow instructions with human\\nfeedback, 2022.\\n[19] Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and\\nChelsea Finn. Direct preference optimization: Your language model is secretly a reward model,\\n2024.\\n[20] Harrison Lee, Samrat Phatale, Hassan Mansoor, Thomas Mesnard, Johan Ferret, Kellie Lu,\\nColton Bishop, Ethan Hall, Victor Carbune, Abhinav Rastogi, and Sushant Prakash. Rlaif vs.\\nrlhf: Scaling reinforcement learning from human feedback with ai feedback, 2024.\\n[21] Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu,\\nChengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu,\\nJianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu,\\nKeqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji\\nLin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang\\nSu, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5\\ntechnical report, 2025.\\n[22] MAA. American invitational mathematics examination - aime. In American Invitational\\nMathematics Examination - AIME 2024, February 2024.\\n[23] Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li,\\nChenjun Xiao, Chenzhuang Du, Chonghua Liao, Chuning Tang, Congcong Wang, Dehao Zhang,\\nEnming Yuan, Enzhe Lu, Fengxiang Tang, Flood Sung, Guangda Wei, Guokun Lai, Haiqing\\nGuo, Han Zhu, Hao Ding, Hao Hu, Hao Yang, Hao Zhang, Haotian Yao, Haotian Zhao, Haoyu\\nLu, Haoze Li, Haozhen Yu, Hongcheng Gao, Huabin Zheng, Huan Yuan, Jia Chen, Jianhang\\nGuo, Jianlin Su, Jianzhou Wang, Jie Zhao, Jin Zhang, Jingyuan Liu, Junjie Yan, Junyan Wu,\\nLidong Shi, Ling Ye, Longhui Yu, Mengnan Dong, Neo Zhang, Ningchen Ma, Qiwei Pan,\\nQucheng Gong, Shaowei Liu, Shengling Ma, Shupeng Wei, Sihan Cao, Siying Huang, Tao\\nJiang, Weihao Gao, Weimin Xiong, Weiran He, Weixiao Huang, Wenhao Wu, Wenyang He,\\nXianghui Wei, Xianqing Jia, Xingzhe Wu, Xinran Xu, Xinxing Zu, Xinyu Zhou, Xuehai Pan,\\nY. Charles, Yang Li, Yangyang Hu, Yangyang Liu, Yanru Chen, Yejie Wang, Yibo Liu, Yidao\\nQin, Yifeng Liu, Ying Yang, Yiping Bao, Yulun Du, Yuxin Wu, Yuzhi Wang, Zaida Zhou,\\nZhaoji Wang, Zhaowei Li, Zhen Zhu, Zheng Zhang, Zhexu Wang, Zhilin Yang, Zhiqi Huang,\\nZihao Huang, Ziyao Xu, and Zonghan Yang. Kimi k1.5: Scaling reinforcement learning with\\nIlms, 2025.\\n[24] Yuyan Zhou, Liang Song, Bingning Wang, and Weipeng Chen. Metagpt: Merging large\\nlanguage models using model exclusive task arithmetic, 2024.\\n[25] Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi,\\nLuke Zettlemoyer, Percy Liang, Emmanuel Cand\u00e8s, and Tatsunori Hashimoto. s1: Simple\\ntest-time scaling, 2025.\\n[26] Liang Wen, Yunke Cai, Fenrui Xiao, Xin He, Qi An, Zhenyu Duan, Yimin Du, Junchen Liu,\\nLifu Tang, Xiaowei Lv, Haosheng Zou, Yongchao Deng, Shousheng Jia, and Xiangzheng Zhang.\\nLight-r1: Curriculum sft, dpo and rl for long cot from scratch and beyond, 2025.\\n[27] Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi\\nHu, Xu Han, Yujie Huang, Yuxiang Zhang, Jie Liu, Lei Qi, Zhiyuan Liu, and Maosong Sun.\\nOlympiadbench: A challenging benchmark for promoting agi with olympiad-level bilingual\\nmultimodal scientific problems, 2024.\\n12\\n\\n[28] Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay\\nRamasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam\\nNeyshabur, Guy Gur-Ari, and Vedant Misra. Solving quantitative reasoning problems with\\nlanguage models, 2022.\\n13\\n\\nA Training Details\\nFor both models, we selected 2,500 problems from the MixMathematics as training data. For each\\nproblem, we sample 12 times. From each set of solutions, we randomly selected 2 solutions for\\ntraining. After computing the rewards, we normalized the reward values. Both models are trained\\nwith 8 * A800-80G GPUs. The other hyperparameters used in the training process are presented in\\nthe table below.\\n\\nTable 4: Hyperparameters for the Deepseek-Distill-1.5B and Deepseek-Distill-7B.\\nHyperparameter Deepseek-Distill-1.5B Deepseek-Distill-7B.\\ncutoff_len 4096 4096\\nbatch_size 32 32\\nlearning_rate 5.0e-7 5.0e-7\\nnum_train_epochs 2.0 2.0\\nlr_scheduler_type constant constant\\nM1 4 4\\nM2 2 2\\nbeta 0.05 0.1\\n\\nB Further Evalution of Different Methods\\nWe further evaluate the performance and efficiency of different methods (AdaR1, DPO, O1-Pruner)\\nacross varying levels of problem difficulty, as illustrated in Figure 5 and Figure 6. Figure Figure 5\\npresents the accuracy ratio of each method relative to a baseline model across different difficulty\\nlevels within the MATH dataset. The results indicate that while performance trends may vary, our\\nproposed AdaR1 method demonstrates strong robustness. Specifically, as the inherent difficulty of\\nthe mathematical problems increases, AdaR1 is able to consistently maintain a high accuracy ratio.\\n\\nFigure 6 show the ratio of average tokens consumed by each method to solve problems across the\\nsame difficulty spectrum. As expected, solving more difficult problems generally requires more\\nreasoning steps and thus more tokens. However, Figure 2 reveals that AdaR1 exhibits favorable\\ntoken efficiency. Critically, when faced with increasing problem difficulty, AdaR1 manages to solve\\nthese complex problems while utilizing relatively fewer tokens compared to other evaluated methods,\\nshowcasing its ability to achieve efficient reasoning even for demanding tasks.\\n\\nCollectively, these figures highlight AdaR1\\'s ability to strike a beneficial balance between accuracy\\nand efficiency. It not only maintains high performance on challenging problems (Figure 5) but also\\ndoes so in a computationally efficient manner, particularly evident in its lower token usage for difficult\\ninstances (Figure 6), addressing limitations observed in prior methods.\\n\\nC Case Study\\n14\\n\\nFigure 5: The ratio of accuracy at different MATH levels on different models. As the difficulty\\nincreases, AdaR1 is able to maintain high accuracy.\\n\\nFigure 6: The ratio of average tokens on different models. As the difficulty increases, AdaR1 is able\\nto use relatively fewer tokens to solve difficult problems.\\n15\\n\\nQuestion: Convert the point (0,3) in rectangular coordinates to polar coordinates.\\nEnter your answer in the form (r, \u03b8), where r > 0 and 0 \u2264 \u03b8 < 2\u03c0. Let\\'s think step by\\nstep and output the final answer within boxed{}. Ground Truth: (3,1)\\nAdaR1: To convert the rectangular coordinate (0, 3) to polar coordinates, we use the formulas:\\nr = \u221ax2 + y2 and \u03b8 = arctan(2). For the point (0, 3): - r = \u221a02 + 32 = \u221a9 = 3 - Since\\nthe point lies on the positive y-axis, the angle \u03b8 is 7. Thus, the polar coordinates are (3,5).\\nFinal Answer:\\n\u03c0\\n(3,5)\\n2\\n\u2713\\nR1: Okay, so I need to convert the rectangular coordinate (0, 3) to polar coordinates. Hmm,\\npolar coordinates are represented as (r, \u03b8), where r is the radius (distance from the origin)\\nand @ is the angle made with the positive x-axis. Let me recall the formulas to convert from\\nrectangular to polar coordinates.\\nFirst, I remember that the radius r can be found using the Pythagorean theorem. Since in\\nrectangular coordinates, the point is (x, y), the distance from the origin is r = \u221ax2 + y\u00b2. So\\nin this case, x is 0 and y is 3. Plugging that into the formula, r should be \u221a02 + 32 which is\\n\u221a0 + 9 = \u221a9 = 3. Okay, that seems straightforward.\\nNow, the angle \u03b8. I know that @ is calculated using the arctangent function because tan 0 = .\\nBut wait, in this case, x is 0. Hmm, arctangent of. But dividing by zero is undefined. I\\nneed to think about what angle e corresponds to when x is 0.\\nY\\nx\\nIn rectangular coordinates, when x is 0, the point lies on the y-axis. Since y is positive\\nhere (y = 3), the point is on the positive y-axis. So, in terms of polar coordinates, the\\nangle @ should be the angle that points directly upwards along the y-axis. I think that\\'s\\nradians, which is 90 degrees. Because starting from the positive x-axis, if we rotate 17 radians\\ncounterclockwise, we end up pointing straight up along the positive y-axis.\\nLet me double-check that. If 0 is, then converting back to rectangular coordinates, x should\\nber cos 0 = 3 cos(17) and y should be r sin 0 = 3 sin().cos() is 0 and sin() is 1, so\\nthat gives x = 0 and y = 3, which is exactly the point we started with. So that makes sense.\\nWait, but could @ also be ? No, because 3 is pointing downward along the negative y-axis,\\nwhich would give us y = \u22123, not 3. So 0 has to be in the range where the point is on the\\npositive y-axis, which is .\\nJust to make sure, let\\'s think about the unit circle. At 0 = 0, we\\'re on the positive x-axis; at\\n0=, positive y-axis; at 0 = \u03c0, negative x-axis; at 0 3, negative y-axis. So yeah, 0 =\\nis correct for the positive y-axis.\\nSo, putting it all together, r is 3 and 6 is . Therefore, the polar coordinates should be (3, \ud50c).\\nLet me just write that out to make sure I didn\\'t make any mistakes.\\nWait, another thought: sometimes people use different ranges for \u03b8, like from \u2013\u03c0 to \u03c0. But in\\nthis case, the problem specifies that @ should be between 0 and 2\u3160, so is within that range.\\nSo, I don\\'t need to adjust @ or anything.\\nJust to recap: the point is (0, 3) in rectangular, so x = 0, y = 3. Calculating r: \u221a02 + 32 = 3.\\nCalculating 0: since x 0 and y is positive, is. So, (3, 1) is the polar coordinate.\\nI think that\\'s solid. Let me visualize it too. If I plot (0,3) on the rectangular coordinate\\nsystem, it\\'s on the y-axis, three units up. In polar coordinates, that\\'s a radius of 3 and an\\nangle of from the positive x-axis. Yep, that lines up perfectly.\\nJust to add another check, let me compute the sine and cosine with 0 = . cos(1) is 0, so\\nx = 3 * 0 = 0, which matches. sin() is 1, so y = 3 * 1 = 3, which also matches. Perfect,\\nno discrepancies there.\\nSo, I think I\\'m confident that the polar coordinates are (3, 1). I don\\'t see any mistakes in my\\nreasoning, and all the checks seem to confirm that this is the correct answer.\\n\u03c0\\nFinal Answer: The polar coordinates are (3,5)\\nFigure 7: AdaR1 can infer answers to simple problems with a small amount of CoT, whereas the R1\\nmodel requires a long CoT.\\n16\\n\\nQuestion: Define p\\n1\\nk=1 k2\\n100\\n=\\n1 and q\\n=\\n78\\n1\\nk=1 k3.\\n1. Find a way to write\\nj=1 k=1 (3+k)3 in terms of p and q. Let\\'s think step by step and output the fi-\\n100\\n100\\n1\\nnal answer within boxed{}. Ground Truth: p \u2212 q\\n100\\n878\\n1\\nAdaR1: Okay, so I have this problem where I need to express a double sum in terms of two\\nknown series, p and q. Let me first write down what\\'s given. We have: p = \u2211k=1 and\\nq = \u2211k=1 . And the expression we need to evaluate is \u2211;=1 k=1 (+6)3. Hmm, so\\nthis is a double sum over j and k starting from 1 to infinity, and each term is 1 divided by\\n(j+k) cubed. I need to find a way to write this in terms of p and q. Let me think about\\nhow to approach this. Maybe I can change variables to simplify the expression. If I let\\nn = j + k, then for each fixed n, j can range from 1 to n-1, and k would be n - j. So, for\\neach n, the number of terms where j + k = n is n - 1. So, maybe I can rewrite the double\\nsum as a single sum over n, and for each n, sum over j from 1 to n-1. Let me write that\\ndown: j=1 k=1 (3+k)3 = 2n=2 j=1 3. Yes, that seems right. Because when j + k =\\nn, j can go from 1 to n-1, and for each j, k is determined as n - j. So, the inner sum is just\\n100\\n1\\n100\\nn-1\\nadding (n-1) times. So, the double sum becomes: 2 31. Simplify that expression:\\n\u03a32 = 2(2-3). Yes, because 1 = - =\\n-. So now, the sum\\n78\\n1\\nn-1\\n18\\nn-1\\nn3\\nn3\\n1\\n100\\ncan be split into two separate sums: ns: \u2211n=272 - \u03a3n=2. 5. But notice that these sums are just\\np and q minus their first term. Let\\'s write that: The first sum is: \u03a3=272 = p - \u2192 = p \u2212 1.\\nSimilarly, the second sum is: \u2211=2 3 = q - 1 = q - 1. Therefore, the original\\n[OMITTED]\\nThus, the original double sum becomes: (p \u2212 1) \u2212 (q - 1) = p \u2212 1-q+1 = p-q. Therefore,\\nthe final answer is: p\\nq =\\n1\\nq\\n100\\n878\\n1\\nR1: Okay, so I have this problem where I need to express a double sum in terms of two\\nknown series, p and q. Let me first write down what\\'s given. We have: p = \u2211k=12 and\\nk=1 3. And the expression we need to evaluate is \u2211j=1 k=1 (j+k)3. Hmm, so this\\nis a double infinite sum where both j and k start from 1 and go to infinity, and each term is 1\\nover (j + k) cubed. I need to find a way to write this in terms of p and q. Let me think about\\nhow to approach this. Maybe I can find a substitution or change variables to simplify the\\nexpression. Since the denominator is (j + k)\u00b3, maybe if I let n = j + k, then I can rewrite the\\nsum in terms of n and another variable. Let\\'s try that. Let me set n = j + k. Then, for each\\nfixed n, j can range from 1 to n - 1, because k would be n - j, which also needs to be at least 1.\\nSo, for each n, j goes from 1 to n - 1, and k = n - j. So, substituting, the double sum becomes:\\n\u03a3=2 =1 3. Wait, that seems promising. Because now for each n, we have a term\\nn3\\nmultiplied by the number of j\\'s, which is (n - 1). So, the inner sum is just 3\u00b9. Therefore,\\nthe entire expression becomes: \u2211n=231. Simplify that: \u2211m=2(2-). Which is:\\n100\\nn-1 1\\n78\\n1\\n10\u221e\\n81\\n100\\n1\\nq\\nn\\nn\\n1\\n\u03a32-23. But I know that: 12 = p and\\nn=1 1 n\u00b3 = q. So, subtracting\\nthe first term from both series: \u2211=2\\n= p-1 1 and\\n2\\n1. Therefore,\\nsubstituting back into the expression: (p \u2212 1) \u2013 (q \u2212 1) = p \u2212 1 \u2212 q + 1 = p - q. Wait, so\\nthe entire double sum simplifies to p - q? Let me verify my steps to make sure I didn\\'t make a\\nmistake. 1. I started by changing variables, setting n = j + k. Then for each n, j goes from 1 to\\nn - 1, so k = n - j also goes from 1 to n - 1. That seems correct. 2. Then, I rewrote the double\\nsum as a single sum over n, with each term being 1. That makes sense because for each n,\\nthere are (n - 1) terms contributing \uacef each. 3. Then, I split the fraction into - . That\\'s\\na valid step because = - = \uae00 - . 4. Then, I separated the sums into two\\ninfinite series starting from n = 2, which is correct because the original sum starts from n = 2.\\n5. Finally, I expressed the sums in terms of p and q by subtracting the n=1 term from each\\nseries. That\\'s correct because the original p and q start from n=1. So, putting it all together,\\nthe double sum equals p - q. That seems neat.But wait, let me double-check. Maybe I can\\ncompute the original double sum numerically for a few terms and see if it approximates to p -\\nq. Let me compute p and q first. We know that... [OMITTED]\\nn-\\nTherefore, the final answer is: p-q\\n1\\n3\\nn\\nn3\\nFigure 8: AdaR1 can infer answers to difficult problems with relatively few long CoT, whereas the\\nR1 model requires longer CoT.\\n17\\n---\\n\\nUSER QUERY:\\nExplain the 3 most important methods introduced in the AdaR1 paper, and also separately the 4 most interesting and surprising insights from the paper')] role='user'",
    "parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text='Based on the provided PDF text about the AdaR1 paper:\\n\\nHere are the three most important methods introduced in the paper:\\n\\n1.  **Constructing a Hybrid Reasoning Model:** The first stage of the framework involves merging a Long-CoT model and a Short-CoT model, typically using a simple linear merging of their parameters ($\\\\theta_H = \\\\alpha\\\\theta_L + (1-\\\\alpha)\\\\theta_S$). This creates a single model capable of generating both long and short reasoning paths, providing the necessary diversity for adaptive reasoning.\\n2.  **Bi-Level Preference Training:** This is the core training strategy in the second stage. It fine-tunes the merged model using preference data at two distinct levels:\\n    *   **Group-Level Preference:** The model is trained to select the appropriate reasoning style (Long-CoT or Short-CoT group) for a given input problem based on its estimated difficulty (measured by the expected accuracy of each group). Preference pairs are constructed by comparing responses from the long and short sampled sets.\\n    *   **Instance-Level Preference:** Within the chosen reasoning style group, the model is further trained to prefer concise and correct reasoning paths. Preference pairs are constructed by comparing responses within the same group, favoring shorter correct responses over longer or incorrect ones.\\n3.  **Sampling and Data Generation:** To enable the Bi-Level Preference Training, the method involves sampling multiple responses from both the Long-CoT and Short-CoT base models for each problem. These samples are then used to determine group-level preferences (based on accuracy expectation) and instance-level preferences (based on correctness and length within the chosen group). This collected data forms the basis for the DPO optimization process.\\n\\nHere are four interesting and surprising insights from the paper:\\n\\n1.  **Varying Benefit of Long-CoT:** An empirical analysis revealed that the benefit of using Long-CoT is highly problem-dependent. For nearly half the samples tested, Long-CoT did not improve accuracy, and in a small fraction, it even decreased performance. It was found to be most beneficial on difficult problems but offered negligible gains or even harmed performance on easier ones.\\n2.  **Limitations of Prior Optimization Methods:** The paper notes that prior methods focusing on reasoning efficiency either primarily reduced redundancy *within* long paths (limited scope) or, if aiming for broader scope (like naive merge or CoT-Valve), suffered significant accuracy degradation because they didn\\'t adapt reasoning depth to problem difficulty.\\n3.  **AdaR1\\'s Adaptive Thinking Ratio:** The \"Thinking Ratio\" study showed that the AdaR1 model adaptively adjusts its propensity for deep thinking (Long-CoT) based on problem difficulty. It exhibits a very low thinking ratio for Level 1 problems in the MATH dataset, which progressively increases for harder levels, reaching its highest for Level 5 tasks.\\n4.  **Efficiency for Difficult Problems:** Despite increasing the thinking ratio for difficult problems, AdaR1 manages to use relatively fewer tokens to solve these complex problems compared to other evaluated methods, demonstrating favorable token efficiency even for demanding tasks, achieving a better balance between accuracy and efficiency.')] role='model'"
  ],
  "current_token_count": 0,
  "prompt_time_counts": [
    0,
    0,
    0,
    0
  ],
  "messages_per_interval": [
    0,
    0,
    1,
    1
  ],
  "_messages_this_interval": 1,
  "active_files": [],
  "thinking_budget": 256
}